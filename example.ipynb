{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tutorial for Pipefunc Package\n",
    "\n",
    "The pipefunc package is a Python library designed to simplify the creation and execution of function pipelines.\n",
    "It allows you to define functions as pipeline steps, automatically managing dependencies and execution order.\n",
    "In this tutorial, we will guide you through the key features of pipefunc, including sequential and parallel execution, map-reduce operations, and advanced functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This page is [a Jupyter notebook](https://github.com/pipefunc/pipefunc/blob/main/example.ipynb), executed and rendered in [the official documentation](https://pipefunc.readthedocs.io/en/latest/tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## High level overview\n",
    "\n",
    "1. The pipefunc package allows to create reusable and callable pipelines.\n",
    "1. A `Pipeline` contains a list of `PipeFunc` objects.\n",
    "1. At its core, these `PipeFunc` objects only contain a function and an output name.\n",
    "1. You can create a `PipeFunc` object directly or using the `@pipefunc` decorator.\n",
    "1. The `Pipeline` will automatically connect all functions based on the output names and function inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building a Simple Pipeline\n",
    "\n",
    "Let's start by importing `pipefunc` and `Pipeline` from the `pipefunc` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import PipeFunc, Pipeline, pipefunc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We then define some functions using the `@pipefunc` decorator.\n",
    "The `@pipefunc` decorator turns these functions into pipeline steps.\n",
    "For each function, we specify an `output_name` which will be used to refer to the output of that function in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipefunc(output_name=\"c\")\n",
    "def f(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"d\")\n",
    "def g(b, c, x=1):  # \"c\" is the output of f\n",
    "    return b * c * x\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"e\")\n",
    "def h(c, d, x=1):  # \"d\" is the output of g\n",
    "    return c * d * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We now have three functions `f`, `g`, and `h`, which we can use to build a pipeline.\n",
    "We create a `Pipeline` object passing the list of functions.\n",
    "We can also enable debugging, profiling, and caching for the entire pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [f, g, h],\n",
    "    debug=True,  # optionally print debug information\n",
    "    profile=True,  # optionally profile the pipeline\n",
    "    cache_type=\"hybrid\",  # optionally cache the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now, we have a pipeline that only requires `a` and `b` as inputs and uses the outputs of the functions and automatically passes them as inputs to the next function.\n",
    "\n",
    "Don't want to use the `@pipefunc` decorator? No problem! You can create a `PipeFunc` object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipefunc(output_name=\"c\")\n",
    "def f(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "# is equivalent to\n",
    "\n",
    "\n",
    "def f(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "f = PipeFunc(f, output_name=\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Visualizing the Pipeline\n",
    "\n",
    "You can visualize your pipeline using the `visualize()` method, and print the nodes in the graph using the `graph.nodes` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    ":::{admonition} Interactive visualization with [`graphviz-anywidget`](https://github.com/pipefunc/graphviz-anywidget)\n",
    ":class: note, dropdown\n",
    "\n",
    "In a live Jupyter notebook, the output below allows interaction with the pipeline visualization.\n",
    " \n",
    "You will be able to zoom by scrolling, pan by dragging the image, and click on nodes to highlight all connected nodes. Click Escape to reset the view.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph nodes:\", pipeline.graph.nodes)\n",
    "pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Executing the Pipeline\n",
    "\n",
    "There are two ways to execute the pipeline:\n",
    "\n",
    "1. Call the pipeline as a function (***sequentially***) and get a specific output:\n",
    "   - `pipeline(output_name, **kwargs)`\n",
    "   - `pipeline.run(output_name, kwargs)`\n",
    "2. Evaluate the entire pipeline (***parallel***) including map-reduce operations:\n",
    "   - `pipeline.map(kwargs)`\n",
    "\n",
    "We start with calling the pipeline directly and then introduce the `map` method.\n",
    "\n",
    "See [this FAQ section](project:faq.md#run-vs-map) for more information on the difference between `run` and `map`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Using `pipeline(...)` (Sequential Execution)\n",
    "\n",
    "If the pipeline has a unique leaf node (single final output), then we can directly call the pipeline object with the input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(a=1, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above returns the output for:\n",
    "pipeline.unique_leaf_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We can also specify the desired output as the first argument of the pipeline function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"`e` is:\", pipeline(\"e\", a=1, b=2))\n",
    "print(\"`d` is:\", pipeline(\"d\", a=1, b=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Using `pipeline.run(...)` (Sequential Execution)\n",
    "\n",
    "Similar to calling the `pipeline` object, we can use the `run` method to execute the pipeline.\n",
    "\n",
    "> Note: The `pipeline(...)` call is just a wrapper around the `run` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.run(\"e\", kwargs={\"a\": 1, \"b\": 2})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "or get _*all*_ function outputs and inputs by specifying `full_output=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.run(\"e\", kwargs={\"a\": 1, \"b\": 2}, full_output=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Using `pipeline.map(...)` (Parallel Execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "`pipeline.map` allows you to execute your pipeline over a set of inputs in parallel.\n",
    "\n",
    "> **Note:** The `mapspec` argument in the `@pipefunc` decorator defines how inputs are mapped to outputs.\n",
    "\n",
    "> **Note:** ⚠️ The mapping computation of the pipeline is done in parallel using the `concurrent.futures.ProcessPoolExecutor` whenever `pipeline.map(..., parallel=True)` (default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipefunc(output_name=\"y\", mapspec=\"x[i] -> y[i]\")\n",
    "def double_it(x: int) -> int:\n",
    "    assert isinstance(x, int)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "pipeline_double = Pipeline([double_it])\n",
    "\n",
    "inputs = {\"x\": [1, 2, 3, 4, 5]}\n",
    "run_folder = \"my_run_folder\"  # save the results in this folder\n",
    "result = pipeline_double.map(inputs, run_folder)\n",
    "print(result[\"y\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "**Syntax of `mapspec`:**\n",
    "\n",
    "```\n",
    "input1[i], input2[j] -> output[i, j]\n",
    "```\n",
    "\n",
    "- **`i` and `j`** are indices over which the function maps.\n",
    "- **`input1[i]`** means the function will receive `input1` at index `i`.\n",
    "- **`output[i, j]`** means the function will produce `output` with indices `i` and `j`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Instead of defining `mapspec` manually, you can use the `add_mapspec_axis` method on the pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take `pipeline` defined above and add a 2D mapspec\n",
    "pipeline2 = pipeline.copy()\n",
    "pipeline2.debug = False  # Turn off debugging print statements\n",
    "pipeline2.add_mapspec_axis(\"a\", axis=\"i\")\n",
    "pipeline2.add_mapspec_axis(\"b\", axis=\"j\")\n",
    "run_folder = \"my_run_folder\"\n",
    "result = pipeline2.map({\"a\": [1, 2], \"b\": [3, 4]}, run_folder, show_progress=True)\n",
    "result[\"e\"].output  # This is now a 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The methods above will automatically generate the `mapspec` for you, which is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2.mapspecs_as_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "The `pipeline.map` method is powerful and can handle complex map-reduce operations, which we will demonstrate next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Map-reduce or fan-in / fan-out operations\n",
    "\n",
    "The script below demonstrates a two-step pipeline: doubling each integer in an input list, followed by summing all the doubled values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.typing import Array\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"y\", mapspec=\"x[i] -> y[i]\")\n",
    "def double_it(x: int) -> int:\n",
    "    assert isinstance(x, int)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"sum\")  # no mapspec, so receives y[:] as input\n",
    "def take_sum(y: Array[int]) -> int:\n",
    "    assert isinstance(y, np.ndarray)\n",
    "    return sum(y)\n",
    "\n",
    "\n",
    "pipeline_map = Pipeline([double_it, take_sum])\n",
    "pipeline_map.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    ":::{admonition} What is `mapspec`?\n",
    ":class: note, dropdown\n",
    "\n",
    "In `double_it`, `mapspec=\"x[i] -> y[i]\"` specifies that each element `i` of the input array `x` is independently processed to produce the corresponding element `i` in the output array `y`.\n",
    "Because `take_sum` does not have a `mapspec`, it receives the entire array `y` for aggregation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Note that the mapspecs are present in the plot. For example, `x` is now `x[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"x\": [0, 1, 2, 3]}\n",
    "run_folder = \"my_run_folder\"\n",
    "results = pipeline_map.map(inputs, run_folder=run_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Check the results in the resulting dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert results[\"y\"].output.tolist() == [0, 2, 4, 6]\n",
    "assert results[\"sum\"].output == 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Or load the outputs from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc.map import load_outputs\n",
    "\n",
    "assert load_outputs(\"y\", run_folder=run_folder).tolist() == [0, 2, 4, 6]\n",
    "assert load_outputs(\"sum\", run_folder=run_folder) == 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Or also load from disk but as an `xarray.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc.map import load_xarray_dataset\n",
    "\n",
    "load_xarray_dataset(run_folder=run_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Advanced features\n",
    "\n",
    "Below are some advanced features of the `pipefunc` package.\n",
    "You will find more features in the [FAQ](https://pipefunc.readthedocs.io/en/latest/faq/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Working with Resources Report\n",
    "\n",
    "The `print_profiling_stats()` method of the `pipeline` provides useful information on the performance of the functions in the pipeline such as CPU usage, memory usage, average time, and the number of times each function was called.\n",
    "This feature is only available if `profile=True` when creating the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print the number of times each function was called\n",
    "# CPU, memory, and time usage is also reported\n",
    "pipeline.print_profiling_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "This report can be beneficial in performance tuning and identifying bottlenecks in your pipeline. You can identify which functions are consuming the most resources and adjust your pipeline accordingly.\n",
    "\n",
    "You can also look all the stats directly with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.profiling_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    "### Handling Multiple Outputs\n",
    "\n",
    "Functions can return multiple results at once.\n",
    "The `output_name` argument allows you to specify multiple outputs by providing a tuple of strings.\n",
    "By default, this assumes the output is a `tuple`.\n",
    "However, if you provide a `output_picker` function, you can return any type of object.\n",
    "As long as the output name can be used to get the desired output from the returned object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "# Returns 2 outputs as a tuple: 'c' and 'const'.\n",
    "@pipefunc(output_name=(\"c\", \"const\"))\n",
    "def add_ab(a, b):\n",
    "    return (a + b, 1)\n",
    "\n",
    "\n",
    "def get_dict_output(output, key):\n",
    "    return output[key]\n",
    "\n",
    "\n",
    "# Function that returns a dictionary, output_picker is used\n",
    "# to pick out \"d\" and \"e\".\n",
    "@pipefunc(output_name=(\"d\", \"e\"), output_picker=get_dict_output)\n",
    "def mul_bc(b, c, x=1):\n",
    "    return {\"d\": b * c, \"e\": x}\n",
    "\n",
    "\n",
    "# Function returns an object with attributes 'g' and 'h'.\n",
    "# output_picker is used to pick out 'g' and 'h'.\n",
    "@pipefunc(output_name=(\"g\", \"h\"), output_picker=getattr)\n",
    "def calc_cde(c, d, e, x):\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    return SimpleNamespace(g=c * d * x, h=c + e)\n",
    "\n",
    "\n",
    "# Define a function add_gh with a single output 'i'.\n",
    "@pipefunc(output_name=\"i\")\n",
    "def add_gh(e, g):\n",
    "    return e + g\n",
    "\n",
    "\n",
    "# Create a pipeline with the defined functions and visualize it.\n",
    "pipeline_multiple = Pipeline([add_ab, mul_bc, calc_cde, add_gh])\n",
    "pipeline_multiple.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_multiple(a=1, b=2, x=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Using the `renames` Feature\n",
    "\n",
    "The `renames` attribute in `pipefunc` allows you to rename the inputs and outputs of a function before passing them to the next step in the pipeline.\n",
    "This can be particularly useful when the same function is used multiple times in a pipeline, or when you want to provide more meaningful names to the inputs and outputs.\n",
    "\n",
    "In the example below, we demonstrate how to use the `renames` attribute to rename the inputs of a function before they are passed to the next step in the pipeline.\n",
    "\n",
    "> ⚠️ Instead of using the `@pipefunc` decorator (which creates `pipefunc.PipeFunc` object), we will create `PipeFunc` objects directly and specify the `renames` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import PipeFunc, Pipeline\n",
    "\n",
    "\n",
    "def prod(a, b):\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# We're going to use these functions multiple times in the pipeline\n",
    "functions = [\n",
    "    PipeFunc(prod, output_name=\"prod1\"),\n",
    "    PipeFunc(prod, output_name=\"prod2\", renames={\"a\": \"x\", \"b\": \"y\"}),\n",
    "    PipeFunc(subtract, output_name=\"delta1\", renames={\"a\": \"prod1\", \"b\": \"prod2\"}),\n",
    "    PipeFunc(subtract, output_name=\"delta2\", renames={\"a\": \"prod2\", \"b\": \"prod1\"}),\n",
    "    PipeFunc(prod, output_name=\"result\", renames={\"a\": \"delta1\", \"b\": \"delta2\"}),\n",
    "]\n",
    "pipeline_renames = Pipeline(functions)\n",
    "\n",
    "inputs = {\"a\": 1, \"b\": 2, \"x\": 3, \"y\": 4}\n",
    "results = pipeline_renames(\"result\", **inputs)\n",
    "\n",
    "# Output the results\n",
    "print(\"Results:\", results)\n",
    "\n",
    "pipeline_renames.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "1. **Function Definitions**:\n",
    "\n",
    "   - `prod(a, b)`: Multiples two numbers and returns the result.\n",
    "   - `subtract(a, b)`: Subtracts `b` from `a` and returns the result.\n",
    "\n",
    "2. **Pipeline Construction**:\n",
    "\n",
    "We are just using the `prod` and `subtract` functions multiple times, but change the names of the inputs and outputs to create a pipeline from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "One can also apply the renames afterwards using the `update_renames` method. Or even to the entire pipeline, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_renames2 = pipeline_renames.copy()\n",
    "pipeline_renames2.update_renames(\n",
    "    {\n",
    "        \"a\": \"aa\",\n",
    "        \"b\": \"bb\",\n",
    "        \"x\": \"xx\",\n",
    "        \"y\": \"yy\",\n",
    "        \"result\": \"final_result\",  # Rename the `output_name` of the last function\n",
    "    },\n",
    "    update_from=\"current\",  # update from the current renames, not the original\n",
    ")\n",
    "pipeline_renames2(aa=1, bb=2, xx=3, yy=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Also check out these `Pipeline` methods:\n",
    "\n",
    "- `Pipeline.update_defaults`\n",
    "- `Pipeline.update_bound`\n",
    "\n",
    "and these `PipeFunc` methods:\n",
    "\n",
    "- `PipeFunc.update_renames`\n",
    "- `PipeFunc.update_defaults`\n",
    "- `PipeFunc.update_bound`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Custom Parallelism\n",
    "\n",
    "By default when `pipeline.map(..., parallel=True)` is used, the pipeline is executed in parallel using the `concurrent.futures.ProcessPoolExecutor`. However, you can also specify a custom executor to control the parallelism of the pipeline execution.\n",
    "\n",
    "It works with any custom executor that has the `concurrent.futures.Executor` interface, so for example it works with:\n",
    "\n",
    "- `concurrent.futures.ProcessPoolExecutor`\n",
    "- `concurrent.futures.ThreadPoolExecutor`\n",
    "- `ipyparallel.Client().executor()`\n",
    "- `dask.distributed.Client().get_executor()`\n",
    "- `mpi4py.futures.MPIPoolExecutor()`\n",
    "- `loky.get_reusable_executor()`\n",
    "\n",
    "To just change the number of cores while using the default executor, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"double\", mapspec=\"x[i] -> double[i]\")\n",
    "def double_it(x: int) -> int:\n",
    "    print(f\"{datetime.datetime.now()} - Running double_it for x={x}\")\n",
    "    time.sleep(1)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"half\", mapspec=\"x[i] -> half[i]\")\n",
    "def half_it(x: int) -> int:\n",
    "    print(f\"{datetime.datetime.now()} - Running half_it for x={x}\")\n",
    "    time.sleep(1)\n",
    "    return x // 2\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"sum\")\n",
    "def take_sum(half: np.ndarray, double: np.ndarray) -> int:\n",
    "    print(f\"{datetime.datetime.now()} - Running take_sum\")\n",
    "    return sum(half + double)\n",
    "\n",
    "\n",
    "pipeline_parallel = Pipeline([double_it, half_it, take_sum])\n",
    "inputs = {\"x\": [0, 1, 2, 3]}\n",
    "run_folder = \"my_run_folder\"\n",
    "executor = ProcessPoolExecutor(max_workers=8)  # use 8 processes\n",
    "results = pipeline_parallel.map(\n",
    "    inputs,\n",
    "    run_folder=run_folder,\n",
    "    parallel=True,\n",
    "    executor=executor,\n",
    "    storage=\"shared_memory_dict\",\n",
    ")\n",
    "print(results[\"sum\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "> ⚠️ In this pipeline, `double_it` and `half_it` are doubly parallel; both the map is parallel and the two functions are executed at the same time, note the timestamps and the `sleep()` calls.\n",
    "> See the `visualize()` output to see the structure of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_parallel.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Combining Pipelines\n",
    "\n",
    "Different pipelines can be combined into a single pipeline using the `Pipeline.join` method or the `|` operator.\n",
    "\n",
    "```{note} tl;dr\n",
    "Use `pipeline1 | pipeline2` to join two pipelines.\n",
    "```\n",
    "\n",
    "In cases the output names and arugments do not match up, we can rename the parameters of an entire pipeline using the `update_renames` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"c\")\n",
    "def f(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"d\")\n",
    "def g(b, c, x=1):\n",
    "    return b + c + x\n",
    "\n",
    "\n",
    "pl1 = Pipeline([f, g])\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"e\")\n",
    "def h(cc, dd, xx=2):\n",
    "    return cc + dd + xx\n",
    "\n",
    "\n",
    "pl2 = Pipeline([h])\n",
    "\n",
    "# We now have two pipelines, `pl1` and `pl2`, that we want to combine\n",
    "# into a single pipeline. However, they have different inputs and defaults.\n",
    "# Let's update the renames and defaults of `pl2` to match `pl1`.\n",
    "pl2_ = pl2.copy()\n",
    "pl2_.update_renames({\"cc\": \"c\", \"dd\": \"d\", \"xx\": \"x\"})\n",
    "pl2_.update_defaults({\"x\": 1})\n",
    "combined_pipeline = pl1 | pl2_  # or use `pl1.combine(pl2_)`\n",
    "\n",
    "combined_pipeline.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined pipeline can now be used as a single pipeline\n",
    "result = combined_pipeline(a=2, b=3, x=2)\n",
    "print(result)  # Output: 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Just to see another quick example of combining pipelines (even though it makes no sense to combine these pipelines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pipeline_silly = pipeline_renames | combined_pipeline\n",
    "pipeline_silly.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., if we want to get the output of `result` in the `pipeline` (not the leaf node!):\n",
    "pipeline_silly(\"result\", a=1, b=2, y=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Caching Results\n",
    "\n",
    "To enable caching, simply set the `cache` attribute to `True` for each function.\n",
    "This can be useful to avoid recomputing results when calling the same function with the same arguments multiple times.\n",
    "\n",
    "```{note}\n",
    "Some cache types support shared memory, which means that the cache can be shared between different processes when running in parallel.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipefunc(output_name=\"y\", cache=True)\n",
    "def my_function(a: int, b: int) -> int:\n",
    "    time.sleep(1)  # Pretend this is a slow function\n",
    "    print(\"Function is called!\")\n",
    "    return a + b\n",
    "\n",
    "\n",
    "# multiple cache_type options are available, e.g., \"lru\", \"hybrid\", \"disk\", and \"simple\"\n",
    "pipeline_cache = Pipeline([my_function], cache_type=\"lru\")\n",
    "\n",
    "# lets call the function 10 times with the same arguments\n",
    "for _ in range(10):\n",
    "    pipeline_cache(a=2, b=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cache object: {pipeline_cache.cache}\")\n",
    "pipeline_cache.cache.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "The cache is populated _**even when using parallel execution**_. To see the cache, you can use the `cache` attribute on the pipeline.\n",
    "\n",
    "```{note}\n",
    "If calling the pipeline like a function (in contrast to using `pipeline.map`) keys of the cache are always in terms of the root arguments of the pipeline. When using `pipeline.map`, the keys are in terms of the arguments of the function.\n",
    "\n",
    "The key is constructed from the function name and the (root) arguments passed to the function. If the arguments are not hashable, the {class}`pipefunc.cache.to_hashable` function is used to *attempt* to convert them to a hashable form.\n",
    "```\n",
    "\n",
    "One can also enable caching after the pipeline is created by setting the `cache` attribute to `True` for each function.\n",
    "\n",
    "```python\n",
    "for f in pipeline.functions:\n",
    "    f.cache = True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Function Argument Combinations\n",
    "\n",
    "As we showed in the first example, we can call the functions in the pipeline by either providing the root inputs or by providing the output of the previous function ourselves.\n",
    "\n",
    "To see all the possible combinations of arguments that can be passed to each function, you can use the `all_arg_combinations` property. This will return a dictionary, with function output names as keys and sets of argument tuples as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_args = pipeline.all_arg_combinations\n",
    "assert all_args == {\n",
    "    # means we can call `pipeline(\"c\", a=1, b=2)`\n",
    "    \"c\": {(\"a\", \"b\")},\n",
    "    # means we can call `pipeline(\"d\", a=1, b=2, x=3)` or `pipeline(\"d\", b=2, c=3, x=4)`\n",
    "    \"d\": {(\"a\", \"b\", \"x\"), (\"b\", \"c\", \"x\")},\n",
    "    # means we can call `pipeline(\"e\", a=1, b=2, x=3)` or `pipeline(\"e\", b=2, d=3, x=4)`, etc.\n",
    "    \"e\": {(\"a\", \"b\", \"x\"), (\"a\", \"b\", \"d\", \"x\"), (\"b\", \"c\", \"x\"), (\"c\", \"d\", \"x\")},\n",
    "}\n",
    "# We can get root arguments for a specific function\n",
    "assert pipeline.root_args(\"e\") == (\"a\", \"b\", \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### More `mapspec` Examples\n",
    "\n",
    "This section shows additional `mapspec` examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### Cross-product of two inputs\n",
    "\n",
    "This example shows how to compute the outer product of two input vectors (`x` and `y`) and then aggregate the resulting matrix along rows, and finally reduce the computation to a single `float` by taking the `norm` of the resulting `aggregated` vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"z\", mapspec=\"x[i], y[j] -> z[i, j]\")\n",
    "def multiply_elements(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"aggregated\", mapspec=\"z[i, :] -> aggregated[i]\")\n",
    "def aggregate_rows(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Sum the elements of each row in matrix z.\"\"\"\n",
    "    return np.sum(z)\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"norm\")\n",
    "def compute_norm(aggregated: np.ndarray) -> float:\n",
    "    \"\"\"Compute the Euclidean norm of the vector aggregated.\"\"\"\n",
    "    return np.linalg.norm(aggregated)\n",
    "\n",
    "\n",
    "pipeline_norm = Pipeline([multiply_elements, aggregate_rows, compute_norm])\n",
    "inputs = {\"x\": [1, 2, 3], \"y\": [4, 5, 6]}\n",
    "results = pipeline_norm.map(inputs, run_folder=\"my_run_folder\")\n",
    "print(\"Norm of the aggregated sums:\", results[\"norm\"].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_norm.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "1. **Matrix Creation (`multiply_elements`)**:\n",
    "\n",
    "   - Each combination of elements from arrays `x` and `y` is multiplied to form the matrix `z`. The `mapspec` `\"x[i], y[j] -> z[i, j]\"` ensures that every pair of elements is processed to generate a 2D matrix.\n",
    "\n",
    "2. **Row Aggregation (`aggregate_rows`)**:\n",
    "\n",
    "   - The matrix `z` is then processed row by row to sum the values, creating an aggregated result for each row. The `mapspec` `\"z[i, :] -> aggregated[i]\"` directs the pipeline to apply the summation across each row, transforming a 2D array into a 1D array of row sums.\n",
    "\n",
    "3. **Vector Norm Calculation (`compute_norm`)**:\n",
    "   - Finally, the norm of the aggregated vector is computed, providing a single scalar value that quantifies the magnitude of the vector formed from row sums. This step does not require a `mapspec` as it takes the entire output from the previous step and produces a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "\n",
    "#### Dynamic Output Shapes and `internal_shapes`\n",
    "\n",
    "In most cases, `pipefunc` automatically infers the output shape of each function based on the `mapspec` and the input shapes.\n",
    "However, use the `internal_shapes` argument if **a function returns an iterable/array that the next function will iterate over using a `mapspec`.**\n",
    "The most common case is when the `mapspec` of the first function is `... -> output1[i]` and the `mapspec` of the second function is `output1[i] -> output2[i]`.\n",
    "\n",
    "**How to use `internal_shapes`:**\n",
    "\n",
    "1. Provide a tuple in `@pipefunc(internal_shape=(...))` representing the shape of the output of that function. You can use `?` for unknown dimensions.\n",
    "2. Provide a dictionary in `pipeline.map(internal_shapes={...})` where keys are function output names, and values are tuples representing the shape *added* by that function. You can use `?` for unknown dimensions.\n",
    "3. Or omit `internal_shapes` and let `pipefunc` infer the shapes automatically (missing out on some consistency checks).\n",
    "\n",
    "**Minimal example:**\n",
    "\n",
    "```python\n",
    "@pipefunc(output_name=\"x\", internal_shape=(10, 20))  # or `internal_shape=(\"?\", \"?\")`\n",
    "def generate_ints() -> np.ndarray:\n",
    "    return np.ones((10, 20))\n",
    "\n",
    "# or\n",
    "\n",
    "pipeline.map(..., internal_shapes={\"x\": (10, 20)})  # or `internal_shapes={\"x\": (\"?\", \"?\")}`\n",
    "```\n",
    "\n",
    "**Full example:**\n",
    "\n",
    "We generate a list of integers with a length determined by an input parameter `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.typing import Array\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"x\")\n",
    "def generate_ints(n: int) -> list[int]:\n",
    "    \"\"\"Generate a list of integers from 0 to n-1.\"\"\"\n",
    "    return list(range(n))\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"y\", mapspec=\"x[i] -> y[i]\")\n",
    "def double_it(x: int) -> int:\n",
    "    \"\"\"Double the input integer.\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"sum\")\n",
    "def take_sum(y: Array[int]) -> int:\n",
    "    \"\"\"Sum a list of integers.\"\"\"\n",
    "    return sum(y)\n",
    "\n",
    "\n",
    "pipeline_sum = Pipeline([generate_ints, double_it, take_sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "Here, `generate_ints` creates a list of length `n`.\n",
    "In the function `double_it`, we map over the resulting list and double each element.\n",
    "Note that PipeFunc automatically generated the `mapspec=\"... -> x[i]\"` for `generate_ints`, which means that the output is an array with index `i` that can be mapped over in the `double_it` function.\n",
    "\n",
    "We indicate that the output is a 1D array with an unknown number of elements by doing either:\n",
    "\n",
    "1. setting the `internal_shape` argument of the `generate_ints` decorator to `@pipefunc(output_name=\"x\", internal_shapes=\"?\")`, or\n",
    "2. by providing a dictionary to the `internal_shapes` argument in `pipeline.map`:\n",
    "\n",
    "Using option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"n\": 4}\n",
    "results = pipeline_sum.map(\n",
    "    inputs,\n",
    "    run_folder=\"my_run_folder\",\n",
    "    internal_shapes={\"x\": (\"?\",)},  # Or if we know the shape of the output `{\"x\": (4,)}`\n",
    ")\n",
    "print(\"Sum of doubled integers:\", results[\"sum\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Or we can omit the `internal_shapes` argument and let `pipefunc` infer the shapes automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline_sum.map(inputs, run_folder=\"my_run_folder\")\n",
    "print(\"Sum of doubled integers:\", results[\"sum\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Zipped inputs\n",
    "\n",
    "This pipeline processes zipped inputs `x` and `y` with independent `z` to compute a function across all combinations, producing a 2D matrix `r`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"r\")\n",
    "def process_elements(x: int, y: int, z: int) -> float:\n",
    "    return x * y + z\n",
    "\n",
    "\n",
    "pipeline_proc = Pipeline([(process_elements, \"x[a], y[a], z[b] -> r[a, b]\")])\n",
    "\n",
    "inputs = {\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8]}\n",
    "\n",
    "results = pipeline_proc.map(inputs, run_folder=\"my_run_folder\")\n",
    "output_matrix = results[\"r\"].output\n",
    "print(\"Output Matrix:\\n\", output_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Function `process_elements`**:\n",
    "\n",
    "  - Takes three inputs: `x`, `y`, and `z`. For each pair `(x[a], y[a])`, the function is applied with each `z[b]`.\n",
    "\n",
    "- **Pipeline Definition**:\n",
    "\n",
    "  - The `mapspec` `\"x[a], y[a], z[b] -> r[a, b]\"` specifies how elements from the inputs are to be combined. It states that each element from the paired inputs `x` and `y` (indexed by `a`) should be processed with each element from `z` (indexed by `b`), resulting in a 2D output array `r`.\n",
    "\n",
    "- **Outputs**:\n",
    "  - The output `r` is a 2-dimensional matrix where the dimensions are determined by the lengths of `x`/`y` and `z`. Each element of this matrix represents the computation result for a specific combination of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Nesting Pipelines for Modularity and Reusability\n",
    "\n",
    "`pipefunc` allows you to create modular and reusable pipeline components by nesting pipelines within each other using the {class}`pipefunc.NestedPipeFunc` class or the {meth}`~pipefunc.Pipeline.nest_funcs` method. This is particularly useful for:\n",
    "\n",
    "- **Encapsulating** a sequence of steps that logically belong together.\n",
    "- **Reusing** a part of a pipeline in multiple projects or within a larger pipeline.\n",
    "- **Abstracting** away internal details of a complex sub-process.\n",
    "- **Selectively avoid returning** intermediate results when using `pipeline.map` (e.g., to prevent serializing large objects and passing it around).\n",
    "\n",
    "**Creating Nested Pipelines:**\n",
    "\n",
    "You can manually create a {class}`~pipefunc.NestedPipeFunc` by passing a list of functions to its constructor.\n",
    "However, a potentially more convenient way is to use the {meth}`pipefunc.Pipeline.nest_funcs` method, which allows you to combine existing functions within a pipeline into a nested one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"c\")\n",
    "def f1(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"d\")\n",
    "def f2(c):\n",
    "    return c * 2\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"e\")\n",
    "def f3(d, x):\n",
    "    return d + x\n",
    "\n",
    "\n",
    "pipeline = Pipeline([f1, f2, f3])\n",
    "\n",
    "# Nest f1 and f2 into a single NestedPipeFunc\n",
    "nested_pipeline = pipeline.copy()\n",
    "nested_func = nested_pipeline.nest_funcs(\n",
    "    {\"c\", \"d\"},\n",
    "    new_output_name=\"d\",  # Only returns \"d\" and not \"c\"\n",
    "    function_name=\"f1_f2\",\n",
    ")\n",
    "nested_pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "This creates a `nested_pipeline` where `f1` and `f2` are combined into a `NestedPipeFunc` named `f1_f2`.\n",
    "The new nested function only returns `\"d\"` and not `\"c\"`.\n",
    "The `new_output_name` must be a subset of the outputs of the nested pipeline.\n",
    "You can optionally specify the name of the function using the `function_name` argument.\n",
    "\n",
    "**Inspecting the Nested Pipeline:**\n",
    "\n",
    "The `nested_func` object contains its own internal pipeline, accessible via the `pipeline` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_func.pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "**Using the Nested Pipeline:**\n",
    "\n",
    "You can now use the `nested_pipeline` like any other pipeline. When executed, the `NestedPipeFunc` will run its internal pipeline, taking the required inputs and producing the specified output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nested_pipeline(a=1, b=2, x=3)\n",
    "print(f\"{result=}\")\n",
    "nested_result = nested_func(a=1, b=2)\n",
    "print(f\"{nested_result=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "**Limitations with `mapspec`:**\n",
    "\n",
    "While `NestedPipeFunc` offers powerful modularity, there are limitations when using it with `mapspec`:\n",
    "\n",
    "- **No Map-Reduce Operations:** The `mapspec` of functions within a `NestedPipeFunc` **cannot** contain reductions (e.g., `x[i, j] -> y[i]`).\n",
    "- **No Dynamic Axis Generation:** The `mapspec` **cannot** dynamically generate new axes (e.g., `... -> out[i]`). In other words, it cannot return an output with an `internal_shape`.\n",
    "- **Allowed `mapspec`s:** You can use mapspecs that do not reduce or create new axes.\n",
    "\n",
    "These limitations stem from the fact that the nested pipeline is treated as a single unit, and its internal operations are not directly exposed to the outer pipeline's mapping logic.\n",
    "\n",
    "**Benefits of Nesting (Despite Limitations):**\n",
    "\n",
    "- **Modularity:** Create self-contained, reusable pipeline components.\n",
    "- **Abstraction:** Hide internal complexity behind a well-defined interface.\n",
    "- **Reusability:** Easily integrate nested pipelines into other projects or larger workflows.\n",
    "- **Clarity:** Improve the overall structure and readability of your pipelines.\n",
    "- **Control over intermediate results:** When using `pipeline.map`, use `nest_funcs` to avoid returning intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "## Full Examples\n",
    "\n",
    "### Example: Physics based example\n",
    "\n",
    "This example demonstrates using the `pipefunc` for a physics-based simulation. The goal is to create a pipeline for geometry creation, meshing, material assignment, and electrostatics calculations, culminating in computing the average charge.\n",
    "\n",
    "> Note: this example is based on the [`aiida-dynamic-workflows` tutorial](https://github.com/microsoft/aiida-dynamic-workflows/blob/4d452ed3be4192dc5b2c8f40690f82c3afcaa7a8/examples/02-workflows.md).\n",
    "\n",
    "We start with defining a few `dataclasses` to represent the geometry, mesh, and material properties. We then define functions to create the geometry, mesh it, assign materials, and calculate the electrostatics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Geometry:\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Mesh:\n",
    "    geometry: Geometry\n",
    "    mesh_size: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Materials:\n",
    "    geometry: Geometry\n",
    "    materials: list[str]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Electrostatics:\n",
    "    mesh: Mesh\n",
    "    materials: Materials\n",
    "    voltages: list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.map import load_outputs\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"geo\")\n",
    "def make_geometry(x: float, y: float) -> Geometry:\n",
    "    return Geometry(x, y)\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"mesh\", \"coarse_mesh\"))\n",
    "def make_mesh(\n",
    "    geo: Geometry,\n",
    "    mesh_size: float,\n",
    "    coarse_mesh_size: float,\n",
    ") -> tuple[Mesh, Mesh]:\n",
    "    return Mesh(geo, mesh_size), Mesh(geo, coarse_mesh_size)\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"materials\")\n",
    "def make_materials(geo: Geometry) -> Materials:\n",
    "    return Materials(geo, [\"i\", \"j\", \"c\"])\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"electrostatics\", mapspec=\"V_left[i], V_right[j] -> electrostatics[i, j]\")\n",
    "def run_electrostatics(\n",
    "    mesh: Mesh,\n",
    "    materials: Materials,\n",
    "    V_left: float,  # noqa: N803\n",
    "    V_right: float,  # noqa: N803\n",
    ") -> Electrostatics:\n",
    "    return Electrostatics(mesh, materials, [V_left, V_right])\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"charge\", mapspec=\"electrostatics[i, j] -> charge[i, j]\")\n",
    "def get_charge(electrostatics: Electrostatics) -> float:\n",
    "    # obviously not actually the charge; but we should return _some_ number that\n",
    "    # is \"derived\" from the electrostatics.\n",
    "    return sum(electrostatics.voltages)\n",
    "\n",
    "\n",
    "# No mapspec: function receives the full 2D array of charges!\n",
    "@pipefunc(output_name=\"average_charge\")\n",
    "def average_charge(charge: np.ndarray) -> float:\n",
    "    return np.mean(charge)\n",
    "\n",
    "\n",
    "pipeline_charge = Pipeline(\n",
    "    [make_geometry, make_mesh, make_materials, run_electrostatics, get_charge, average_charge],\n",
    ")\n",
    "pipeline_charge.visualize(orient=\"TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Let's run the map for some inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"V_left\": np.linspace(0, 2, 3),\n",
    "    \"V_right\": np.linspace(-0.5, 0.5, 2),\n",
    "    \"x\": 0.1,\n",
    "    \"y\": 0.2,\n",
    "    \"mesh_size\": 0.01,\n",
    "    \"coarse_mesh_size\": 0.05,\n",
    "}\n",
    "\n",
    "run_folder = \"my_run_folder\"\n",
    "results = pipeline_charge.map(inputs, run_folder=run_folder, parallel=False)\n",
    "assert results[\"average_charge\"].output == 1.0\n",
    "assert results[\"average_charge\"].output_name == \"average_charge\"\n",
    "assert load_outputs(\"average_charge\", run_folder=run_folder) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "This example highlighted how to run a simulation that included a map-reduce operation.\n",
    "Often we want to sweep this over multiple parameters.\n",
    "You could add all the `mapspec`s required to map an additional parameter.\n",
    "Alternatively, you can use the `pipeline.add_mapspec_axis` method to add an axis to parameters of the pipeline.\n",
    "\n",
    "See the example below, where we extend the `mapspec`s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a cross-product of x and y\n",
    "pipeline_charge.add_mapspec_axis(\"x\", axis=\"a\")\n",
    "pipeline_charge.add_mapspec_axis(\"y\", axis=\"b\")\n",
    "\n",
    "# And also a cross-product of the zipped mesh_size and coarse_mesh_size\n",
    "pipeline_charge.add_mapspec_axis(\"mesh_size\", axis=\"c\")\n",
    "pipeline_charge.add_mapspec_axis(\"coarse_mesh_size\", axis=\"c\")\n",
    "\n",
    "# Finally, the mapspecs become, which shows a 3D array for the `average_charge`:\n",
    "pipeline_charge.mapspecs_as_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_charge.visualize(orient=\"TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "Let's run it on a 2x2x2 grid of inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"V_left\": np.linspace(0, 2, 3),\n",
    "    \"V_right\": np.linspace(-0.5, 0.5, 2),\n",
    "    \"x\": np.linspace(0.1, 0.2, 2),\n",
    "    \"y\": np.linspace(0.2, 0.3, 2),\n",
    "    \"mesh_size\": [0.01, 0.02],\n",
    "    \"coarse_mesh_size\": [0.05, 0.06],\n",
    "}\n",
    "results = pipeline_charge.map(inputs, run_folder=run_folder, parallel=False)\n",
    "output = results[\"average_charge\"].output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "We can also load all data as `xarray.Dataset`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc.map import load_xarray_dataset\n",
    "\n",
    "ds = load_xarray_dataset(run_folder=run_folder)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "Or specify the `output_name` to load only specific outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_charge = load_xarray_dataset(\"average_charge\", run_folder=run_folder)\n",
    "avg_charge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "Now imagine that the electrostatics object is a very large object that we cannot afford to save and load from disk.\n",
    "For this purpose there is the `pipfunc.NestedPipeFunc` class that allows to combine multiple functions into a single function. We can then tell it to not return the output of the intermediate functions by specifying which outputs to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_charge2 = pipeline_charge.copy()\n",
    "nested_func = pipeline_charge2.nest_funcs(\n",
    "    {\"electrostatics\", \"charge\"},\n",
    "    new_output_name=\"charge\",\n",
    "    # We can also specify `(\"charge\", \"electrostatics\")` to get both outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "This `nested_func` contains an internal pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_func.pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "When visualizing the pipeline, you can see that the `NestedFunc` is shown as a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_charge2.visualize(orient=\"TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "### Example: Sensor Data Processing Pipeline\n",
    "\n",
    "```{note}\n",
    "This example uses `scipy` and `seaborn` libraries for data processing and visualization. Make sure to install these libraries before running the code.\n",
    "```\n",
    "\n",
    "Let's create a detailed example for the sensor data processing pipeline. This example will simulate the following steps:\n",
    "\n",
    "1. **Data Collection**: Simulate raw sensor data.\n",
    "2. **Noise Filtering**: Apply a basic noise filter.\n",
    "3. **Feature Extraction**: Extract features such as peak values.\n",
    "4. **Anomaly Detection**: Identify anomalies within the extracted features.\n",
    "5. **Visualization**: Plot the results.\n",
    "\n",
    "Here’s how this pipeline can be implemented using `pipefunc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "# Step 1: Simulate Sensor Data\n",
    "@pipefunc(output_name=\"raw_data\")\n",
    "def collect_data(num_samples: int = 1000, noise_level: float = 0.1):\n",
    "    time = np.linspace(0, 100, num_samples)\n",
    "    data = np.sin(time) + noise_level * np.random.randn(num_samples)\n",
    "    return time, data\n",
    "\n",
    "\n",
    "# Step 2: Noise Filtering\n",
    "@pipefunc(output_name=\"filtered_data\")\n",
    "def filter_noise(raw_data):\n",
    "    time, data = raw_data\n",
    "    # Simple moving average filter\n",
    "    window_size = 5\n",
    "    filtered_data = np.convolve(data, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "    adjusted_time = time[: len(filtered_data)]\n",
    "    return adjusted_time, filtered_data\n",
    "\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "@pipefunc(output_name=(\"peak_times\", \"peak_values\"))\n",
    "def extract_features(filtered_data):\n",
    "    time, data = filtered_data\n",
    "    # Find peaks in the data\n",
    "    peaks, _ = find_peaks(data, height=0)\n",
    "    peak_times = time[peaks]\n",
    "    peak_values = data[peaks]\n",
    "    return peak_times, peak_values\n",
    "\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "@pipefunc(output_name=(\"anomaly_times\", \"anomaly_values\"))\n",
    "def detect_anomalies(peak_times, peak_values, threshold: float = 0.8):\n",
    "    # Simple anomaly detection based on threshold\n",
    "    anomalies = peak_values > threshold\n",
    "    anomaly_times = peak_times[anomalies]\n",
    "    anomaly_values = peak_values[anomalies]\n",
    "    return anomaly_times, anomaly_values\n",
    "\n",
    "\n",
    "# Step 5: Visualization\n",
    "@pipefunc(output_name=\"visualization\")\n",
    "def visualize(raw_data, filtered_data, peak_times, peak_values, anomaly_times, anomaly_values):\n",
    "    raw_time, raw_data = raw_data\n",
    "    filt_time, filt_data = filtered_data\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(raw_time, raw_data, label=\"Raw Data\", alpha=0.5)\n",
    "    plt.plot(filt_time, filt_data, label=\"Filtered Data\")\n",
    "    plt.scatter(peak_times, peak_values, color=\"green\", label=\"Peaks\")\n",
    "    plt.scatter(anomaly_times, anomaly_values, color=\"red\", label=\"Anomalies\")\n",
    "    plt.title(\"Sensor Data Processing\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Sensor Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_sensor = Pipeline(\n",
    "    [collect_data, filter_noise, extract_features, detect_anomalies, visualize],\n",
    ")\n",
    "\n",
    "pipeline_sensor.visualize(orient=\"TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full pipeline\n",
    "pipeline_sensor(\"visualization\", num_samples=1000, noise_level=0.1, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Data Collection (`collect_data`)**: Simulate time and sine wave data with added Gaussian noise.\n",
    "- **Noise Filtering (`filter_noise`)**: Use a simple moving average to smooth the data.\n",
    "- **Feature Extraction (`extract_features`)**: Find peaks in the filtered data using `scipy.signal.find_peaks`.\n",
    "- **Anomaly Detection (`detect_anomalies`)**: Identify peaks above a certain threshold as anomalies.\n",
    "- **Visualization (`visualize`)**: Plot raw data, filtered data, detected peaks, and anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "**Do a study for different noise levels and thresholds:**\n",
    "\n",
    "We can expand the analysis by examining how varying levels of noise and different sample sizes affect the detection of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline that terminates at the anomaly detection step (so without visualization)\n",
    "pipeline_sensor2 = pipeline_sensor.subpipeline(output_names={\"anomaly_times\", \"anomaly_values\"})\n",
    "\n",
    "# Also let's add a function to get the number of detected anomalies\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"num_anomalies\")\n",
    "def count_anomalies(anomaly_times):\n",
    "    return len(anomaly_times)\n",
    "\n",
    "\n",
    "pipeline_sensor2.add(count_anomalies)\n",
    "\n",
    "# Add dimensional axes to the input parameters\n",
    "pipeline_sensor2.add_mapspec_axis(\"num_samples\", axis=\"i\")\n",
    "pipeline_sensor2.add_mapspec_axis(\"noise_level\", axis=\"j\")\n",
    "\n",
    "# Run the subpipeline with different configurations\n",
    "result = pipeline_sensor2.map(\n",
    "    inputs={\"num_samples\": [1000, 500, 1000], \"noise_level\": [0.05, 0.1, 0.2]},\n",
    "    run_folder=\"sensor_map_results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "**Plotting Results for Different Noise Levels and Thresholds:**\n",
    "\n",
    "To better understand the relationships and impacts of noise and sample size on anomaly detection, visualize the results with a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize the resulting xarray dataset\n",
    "import seaborn as sns\n",
    "\n",
    "from pipefunc.map import load_xarray_dataset\n",
    "\n",
    "ds = load_xarray_dataset(\"num_anomalies\", run_folder=\"sensor_map_results\")\n",
    "\n",
    "# Convert data variables to a numpy array for plotting\n",
    "num_anomalies_data = ds[\"num_anomalies\"].data.astype(int)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    num_anomalies_data,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    xticklabels=ds[\"noise_level\"].values,\n",
    "    yticklabels=ds[\"num_samples\"].values,\n",
    ")\n",
    "\n",
    "# Add labels\n",
    "plt.title(\"Number of Anomalies Heatmap\")\n",
    "plt.xlabel(\"Noise Level\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "### Example: Image Processing Workflow Example with `mapspec`\n",
    "\n",
    "```{note}\n",
    "This example uses `scikit-image` for image processing. If you don't have it installed, you can install it using `pip install scikit-image`.\n",
    "```\n",
    "\n",
    "In this example, we'll process a batch of images to:\n",
    "\n",
    "1. **Load and Preprocess**: Convert each image to grayscale to reduce complexity and prepare it for segmentation.\n",
    "2. **Image Segmentation**: Detect regions of interest within each individual image using an edge detection technique.\n",
    "3. **Feature Extraction**: Identify and count the number of detected regions for each processed image.\n",
    "4. **Classification**: Classify each image as \"Complex\" or \"Simple\" based on the extracted features.\n",
    "5. **Result Aggregation**: Summarize the classification results across all images in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import data, filters, measure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "# Step 1: Image Loading and Preprocessing\n",
    "@pipefunc(output_name=\"gray_image\", mapspec=\"image[n] -> gray_image[n]\")\n",
    "def load_and_preprocess_image(image):\n",
    "    return rgb2gray(image)\n",
    "\n",
    "\n",
    "# Step 2: Image Segmentation\n",
    "@pipefunc(output_name=\"segmented_image\", mapspec=\"gray_image[n] -> segmented_image[n]\")\n",
    "def segment_image(gray_image):\n",
    "    return filters.sobel(gray_image)\n",
    "\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "@pipefunc(output_name=\"feature\", mapspec=\"segmented_image[n] -> feature[n]\")\n",
    "def extract_feature(segmented_image):\n",
    "    boundaries = find_boundaries(segmented_image > 0.1)\n",
    "    labeled_image = measure.label(boundaries)\n",
    "    num_regions = np.max(labeled_image)\n",
    "    return {\"num_regions\": num_regions}\n",
    "\n",
    "\n",
    "# Step 4: Object Classification\n",
    "@pipefunc(output_name=\"classification\", mapspec=\"feature[n] -> classification[n]\")\n",
    "def classify_object(feature):\n",
    "    # Classify image as 'Complex' if the number of regions is above a threshold.\n",
    "    classification = \"Complex\" if feature[\"num_regions\"] > 5 else \"Simple\"\n",
    "    return classification\n",
    "\n",
    "\n",
    "# Step 5: Result Aggregation\n",
    "@pipefunc(output_name=\"summary\")\n",
    "def aggregate_results(classification):\n",
    "    simple_count = sum(1 for c in classification if c == \"Simple\")\n",
    "    complex_count = len(classification) - simple_count\n",
    "    return {\"Simple\": simple_count, \"Complex\": complex_count}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_img = Pipeline(\n",
    "    [\n",
    "        load_and_preprocess_image,\n",
    "        segment_image,\n",
    "        extract_feature,\n",
    "        classify_object,\n",
    "        aggregate_results,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Simulate a batch of images (using built-in scikit-image sample images)\n",
    "images = [\n",
    "    data.astronaut(),\n",
    "    data.coffee(),\n",
    "    data.coffee(),\n",
    "]  # Repeat the coffee image to simulate multiple images\n",
    "\n",
    "# Run the pipeline on the images\n",
    "results_summary = pipeline_img.map({\"image\": images})\n",
    "print(\"Classification Summary:\", results_summary[\"summary\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Image Loading and Preprocessing (`load_and_preprocess_image`)**: Converts each individual image to grayscale, ensuring independent processing via `mapspec`.\n",
    "- **Image Segmentation (`segment_image`)**: Applies Sobel filtering to detect edges and regions of interest in each grayscale image, taking advantage of parallel processing for the batch.\n",
    "- **Feature Extraction (`extract_feature`)**: Identifies boundaries and counts distinct regions in each segmented image, returning the count as a feature for classification.\n",
    "- **Object Classification (`classify_object`)**: Classifies each image as \"Complex\" or \"Simple\" based on the detected regions relative to a predefined threshold.\n",
    "- **Result Aggregation (`aggregate_results`)**: Aggregates classifications to provide a summary of \"Simple\" and \"Complex\" images across the batch.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- **`mapspec`**: Enables independent and parallel processing of each image by defining input-to-output mappings, removing the need for explicit parallel code.\n",
    "- **Functional Structure**: Utilizes `pipefunc` to manage dependencies and efficiently execute batch image processing, highlighting the framework's ability to handle complex workflows.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "### Example: Natural Language Processing Pipeline for Text Summarization\n",
    "\n",
    "```{note}\n",
    "This example uses `nltk` for text processing. If you don't have it installed, you can install it using `pip install nltk`.\n",
    "```\n",
    "\n",
    "Let's create a simplified NLP workflow for text summarization with steps involving tokenization, keyword extraction, summary generation, and sentiment analysis. This example will demonstrate the use of `pipefunc` to handle dependencies and illustrate `mapspec` by processing multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "\n",
    "# Step 1: Text Tokenization\n",
    "@pipefunc(output_name=\"tokens\", mapspec=\"text[n] -> tokens[n]\")\n",
    "def tokenize_text(text):\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "# Step 2: Keyword Extraction\n",
    "@pipefunc(output_name=\"keywords\", mapspec=\"tokens[n] -> keywords[n]\")\n",
    "def extract_keywords(tokens):\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    common_keywords = freq_dist.most_common(5)\n",
    "    return [word for word, _ in common_keywords]\n",
    "\n",
    "\n",
    "# Step 3: Summary Generation\n",
    "@pipefunc(output_name=\"summary\", mapspec=\"text[n], keywords[n] -> summary[n]\")\n",
    "def generate_summary(text, keywords):\n",
    "    sentences = sent_tokenize(text)\n",
    "    important_sentences = [\n",
    "        sentence\n",
    "        for sentence in sentences\n",
    "        if any(keyword in sentence.lower() for keyword in keywords)\n",
    "    ]\n",
    "    return \" \".join(important_sentences[:2])  # Return the first two important sentences\n",
    "\n",
    "\n",
    "# Step 4: Sentiment Analysis\n",
    "@pipefunc(output_name=\"sentiment\", mapspec=\"summary[n] -> sentiment[n]\")\n",
    "def analyze_sentiment(summary):\n",
    "    # Simplified sentiment analysis: More positive words = Positive sentiment\n",
    "    positive_words = {\"good\", \"great\", \"excellent\", \"positive\", \"fortunate\"}\n",
    "    negative_words = {\"bad\", \"terrible\", \"poor\", \"negative\", \"unfortunate\"}\n",
    "    words = set(summary.lower().split())\n",
    "    sentiment_score = len(words & positive_words) - len(words & negative_words)\n",
    "    if sentiment_score > 0:\n",
    "        return \"Positive\"\n",
    "    if sentiment_score < 0:\n",
    "        return \"Negative\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "\n",
    "# Step 5: Summarization Result Aggregation\n",
    "@pipefunc(output_name=\"result_summary\")\n",
    "def aggregate_summarization(sentiment):\n",
    "    # Convert the sentiment masked array to a list\n",
    "    sentiment_list = np.array(sentiment).tolist()\n",
    "\n",
    "    # Count occurrences of each sentiment type\n",
    "    positive_count = sentiment_list.count(\"Positive\")\n",
    "    negative_count = sentiment_list.count(\"Negative\")\n",
    "    neutral_count = sentiment_list.count(\"Neutral\")\n",
    "\n",
    "    return {\"Positive\": positive_count, \"Negative\": negative_count, \"Neutral\": neutral_count}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_sentiment = Pipeline(\n",
    "    [\n",
    "        tokenize_text,\n",
    "        extract_keywords,\n",
    "        generate_summary,\n",
    "        analyze_sentiment,\n",
    "        aggregate_summarization,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Example texts to summarize\n",
    "texts = [\n",
    "    \"The movie was excellent! The performances were outstanding, and the plot was captivating.\",\n",
    "    \"The movie was bad and boring. I found it dull and slow with no gripping moments.\",\n",
    "    \"An alright film with a good sense of humor but lacking depth in character development.\",\n",
    "]\n",
    "\n",
    "# Run the pipeline on texts\n",
    "results_summary = pipeline_sentiment.map({\"text\": texts}, parallel=True)\n",
    "print(\"Summarization Sentiment Summary:\", results_summary[\"result_summary\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Text Tokenization (`tokenize_text`)**: Tokenizes each text, removing stop words using a `mapspec` for independent processing.\n",
    "- **Keyword Extraction (`extract_keywords`)**: Extracts the most frequent words as keywords for each tokenized text.\n",
    "- **Summary Generation (`generate_summary`)**: Generates a summary by selecting sentences with the most important keywords.\n",
    "- **Sentiment Analysis (`analyze_sentiment`)**: Analyzes sentiment of summary texts using a set of positive and negative keywords for simplicity.\n",
    "- **Result Aggregation (`aggregate_summarization`)**: Aggregates sentiments to provide an overall sentiment summary across all texts.\n",
    "\n",
    "This example showcases the capability of `pipefunc` to manage complex workflows in NLP applications, leveraging `mapspec` to process multiple text inputs efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Weather Simulation and Analysis Pipeline with `xarray`\n",
    "\n",
    "In this example, we'll generate temperature data for multiple cities over several days, compute statistics like mean and variance, and then use `xarray` to load and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.map import load_xarray_dataset\n",
    "\n",
    "\n",
    "# Step 1: Simulate Temperature Data\n",
    "@pipefunc(output_name=\"temperature\", mapspec=\"city[i], day[j] -> temperature[i, j]\")\n",
    "def simulate_temperature(city, day):\n",
    "    np.random.seed(hash(city) % 2**32)  # For reproducibility\n",
    "    mean_temp = 20 + (hash(city) % 10)  # Base temp varies by city\n",
    "    temp_variation = 5 * np.sin(day.dayofyear * (2 * np.pi / 365))  # Seasonal variation\n",
    "    noise = np.random.normal(0, 2)  # Random daily fluctuation\n",
    "    return float(mean_temp + temp_variation + noise)  # Ensure this is a float\n",
    "\n",
    "\n",
    "# Step 2: Compute Statistics\n",
    "@pipefunc(\n",
    "    output_name=(\"mean_temp\", \"variance\"),\n",
    "    mapspec=\"temperature[i, :] -> mean_temp[i], variance[i]\",\n",
    "    output_picker=dict.__getitem__,\n",
    ")\n",
    "def compute_statistics(temperature):\n",
    "    temp_array = np.array(temperature, dtype=float)  # Ensure it's a numeric array\n",
    "    mean_temp = np.mean(temp_array)\n",
    "    var_temp = np.var(temp_array)\n",
    "    return {\"mean_temp\": mean_temp, \"variance\": var_temp}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_weather = Pipeline([simulate_temperature, compute_statistics])\n",
    "\n",
    "# Define cities and days\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "days = pd.date_range(\"2023-01-01\", \"2023-01-10\")  # 10 days\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline_weather.map({\"city\": cities, \"day\": days}, run_folder=\"weather_simulation_results\")\n",
    "\n",
    "# Load and display the xarray dataset\n",
    "weather_dataset = load_xarray_dataset(run_folder=\"weather_simulation_results\")\n",
    "display(weather_dataset)\n",
    "\n",
    "# Plot temperatures for each city\n",
    "weather_dataset.temperature.astype(float).plot.line(\n",
    "    x=\"day\",\n",
    "    hue=\"city\",\n",
    "    marker=\"o\",\n",
    "    figsize=(12, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Temperature Simulation (`simulate_temperature`)**: Each city has its synthetic daily temperature calculated using a sinusoidal pattern and noise. The `mapspec` `city[i], day[j] -> temperature[i, j]` allows us to handle city-by-day combinations automatically.\n",
    "\n",
    "- **Statistics Calculation (`compute_statistics`)**: Computes the mean and variance of the daily temperature, mapping over cities.\n",
    "\n",
    "- **Automatic `xarray.Dataset`**: The `pipeline.map()` call ensures that the data is structured into an N-dimensional format, representing the outputs naturally as an `xarray.Dataset`.\n",
    "\n",
    "- **Retrieving with `load_xarray_dataset`**: Quickly access the results organized by city and day indices without manually constructing them.\n",
    "\n",
    "This showcases `pipefunc`'s powerful ability to manage multi-dimensional computations and data structuring, presenting an efficient workflow for simulating and analyzing temperature variations.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipefunc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
