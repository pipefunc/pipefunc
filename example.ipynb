{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tutorial for Pipefunc Package\n",
    "\n",
    "The `pipefunc` package is a Python library that allows you to define functions as pipelines, with each function providing a single step in the pipeline. In this tutorial, we will explain how to use the package, based on an example notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "For the latest documentation, check out [the official documentation](https://pipefunc.readthedocs.io/en/latest/#what-is-this)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## High level overview\n",
    "\n",
    "1. The pipefunc package allows to create reusable and callable pipelines. \n",
    "1. A `Pipeline` contains a list of `PipeFunc` objects.\n",
    "1. At its core, these `PipeFunc` objects only contain a function and an output name.\n",
    "1. You can create a `PipeFunc` object directly or using the `@pipefunc` decorator.\n",
    "1. The `Pipeline` will automatically connect all functions based on the output names and function inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Building a Simple Pipeline\n",
    "\n",
    "Let's start by importing `pipefunc` and `Pipeline` from the `pipefunc` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We then define some functions using the `@pipefunc` decorator. The `@pipefunc` decorator turns these functions into pipeline steps. For each function, we specify an `output_name` which will be used to refer to the output of that function in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipefunc(output_name=\"c\")\n",
    "def f_c(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"d\")\n",
    "def f_d(b, c, x=1):  # \"c\" is the output of f_c\n",
    "    return b * c\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"e\")\n",
    "def f_e(c, d, x=1):  # \"d\" is the output of f_d\n",
    "    return c * d * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We now have three functions `f_c`, `f_d`, and `f_e`, which we can use to build a pipeline. Let's create a `Pipeline` object, passing our functions in the order we want them to execute. We can also enable debugging, profiling, and caching for the entire pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [f_c, f_d, f_e],\n",
    "    debug=True,  # optionally print debug information\n",
    "    profile=True,  # optionally profile the pipeline\n",
    "    cache_type=\"hybrid\",  # optionally cache the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now, we have a pipeline that adds two numbers (function `f_c`), multiplies two numbers (function `f_d`), and again multiplies two numbers (function `f_e`).\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing the Pipeline\n",
    "\n",
    "You can visualize your pipeline using the `visualize()` method, and print the nodes in the graph using the `graph.nodes` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.visualize()\n",
    "print(\"Graph nodes:\", pipeline.graph.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using the Pipeline\n",
    "\n",
    "There are several ways in which you can use the pipeline.\n",
    "\n",
    "You can:\n",
    "1. Call the pipeline as a function (***sequentially***) and get a specific output:\n",
    "  - `pipeline.run(output_name, kwargs)`\n",
    "  - `pipeline(output_name, **kwargs)`\n",
    "  - `f = pipeline.func(output_name); f(**kwargs)`\n",
    "2. Evaluate the entire pipeline (***parallel***) including map-reduce operations:\n",
    "  - `pipeline.map(kwargs)`\n",
    "\n",
    "We will first demonstrate how to use the pipeline by calling it as a function and later introduce the more powerful `pipeline.map`.\n",
    "\n",
    "See [this FAQ section](project:faq.md#run-vs-map) for more information on the difference between `run` and `map`.\n",
    "\n",
    "### 1. Directly calling the `pipeline` object\n",
    "\n",
    "If the pipeline has a unique leaf node (single final output), then we can directly call the pipeline object with the input arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(a=1, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above returns the output for:\n",
    "pipeline.unique_leaf_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We can also specify the desired output as the first argument of the pipeline function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"`e` is:\", pipeline(\"e\", a=1, b=2))\n",
    "print(\"`d` is:\", pipeline(\"d\", a=1, b=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 3. Using `pipeline.run`\n",
    "\n",
    "Similar to calling the `pipeline` object, we can use the `run` method to execute the pipeline.\n",
    "\n",
    "> Note: The `Pipeline.__call__` method is just a wrapper around the `run` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline.run(output_name=\"e\", kwargs={\"a\": 1, \"b\": 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "or get _*all*_ function outputs and inputs by specifying `full_output=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline.run(output_name=\"e\", kwargs={\"a\": 1, \"b\": 2}, full_output=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 4. Get a function handle for a specific output (`pipeline.func`)\n",
    "\n",
    "We can get a handle for each function using the `func` method on the pipeline, passing the output name of the function we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_d = pipeline.func(\"d\")\n",
    "pf_e = pipeline.func(\"e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We can now use these handles as if they were the original functions. The pipeline will automatically ensure that the functions are called in the correct order, passing the output of one function as the input to the next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = f_c(a=2, b=3)  # call the wrapped function directly\n",
    "assert c == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    f_d(b=3, c=5)\n",
    "    == pf_d(a=2, b=3)  # We can call pf_d with different arguments\n",
    "    == pf_d(b=3, c=5)\n",
    "    == 15\n",
    ")\n",
    "assert pf_e(c=c, d=15, x=1) == pf_e(a=2, b=3, x=1) == pf_e(a=2, b=3, d=15, x=1) == 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "The functions returned by `pipeline.func` have several additional methods\n",
    "\n",
    "#### Using the call_full_output Method\n",
    "\n",
    "The `call_full_output()` method can be used to call the function and get all the outputs from the pipeline as a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_e = pipeline.func(\"e\")\n",
    "pf_e.call_full_output(a=2, b=3, x=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Direct Calling with Root Arguments (as positional arguments)\n",
    "\n",
    "You can directly call the functions in the pipeline with the root arguments using the `call_with_root_args()` method. It automatically executes all the dependencies of the function in the pipeline with the given root arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_e = pipeline.func(\"e\")\n",
    "pf_e.call_with_root_args(1, 2, 1)  # note these are now positional args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "This executes the function `f_e` with the root arguments `a=1, b=2, x=1`.\n",
    "\n",
    "For more information about this method, you can use the Python built-in `help` function or the `?` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pf_e.call_with_root_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "This shows the signature and the doc-string of the `call_with_root_args` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Function Argument Combinations\n",
    "\n",
    "As seen above, we can call the functions in the pipeline by either providing the root inputs or by providing the output of the previous function ourselves.\n",
    "\n",
    "To see all the possible combinations of arguments that can be passed to each function, you can use the `all_arg_combinations` property. This will return a dictionary, with function output names as keys and sets of argument tuples as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_args = pipeline.all_arg_combinations\n",
    "assert all_args == {\n",
    "    # means we can call `pipeline(\"c\", a=1, b=2)`\n",
    "    \"c\": {(\"a\", \"b\")},\n",
    "    # means we can call `pipeline(\"d\", a=1, b=2, x=3)` or `pipeline(\"d\", b=2, c=3, x=4)`\n",
    "    \"d\": {(\"a\", \"b\", \"x\"), (\"b\", \"c\", \"x\")},\n",
    "    # means we can call `pipeline(\"e\", a=1, b=2, x=3)` or `pipeline(\"e\", b=2, d=3, x=4)`, etc.\n",
    "    \"e\": {(\"a\", \"b\", \"x\"), (\"a\", \"b\", \"d\", \"x\"), (\"b\", \"c\", \"x\"), (\"c\", \"d\", \"x\")},\n",
    "}\n",
    "# We can get root arguments for a specific function\n",
    "assert pipeline.root_args(\"e\") == (\"a\", \"b\", \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Handling Multiple Outputs\n",
    "\n",
    "Functions can return multiple results at once. The `output_name` argument allows you to specify multiple outputs by providing a tuple of strings. By default, this assumes the output is a tuple. However, if the output is a single element selected from a tuple, you can use the `output_picker` argument to specify that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "# Define a function add_ab with multiple outputs, 'c' and 'const'.\n",
    "@pipefunc(output_name=(\"c\", \"const\"))\n",
    "def add_ab(a, b):\n",
    "    return (a + b, 1)\n",
    "\n",
    "\n",
    "# Define a function mul_bc with multiple outputs, 'd' and 'e',\n",
    "# where output_picker is used to select the output.\n",
    "@pipefunc(\n",
    "    output_name=(\"d\", \"e\"),\n",
    "    output_picker=dict.__getitem__,\n",
    ")\n",
    "def mul_bc(b, c, x=1):\n",
    "    return {\"d\": b * c, \"e\": x}\n",
    "\n",
    "\n",
    "# Define a function calc_cde with multiple outputs, 'g' and 'h',\n",
    "# where output_picker is used to select the output.\n",
    "@pipefunc(\n",
    "    output_name=(\"g\", \"h\"),\n",
    "    output_picker=getattr,\n",
    ")\n",
    "def calc_cde(c, d, e, x):\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    return SimpleNamespace(g=c * d * x, h=c + e)\n",
    "\n",
    "\n",
    "# Define a function add_gh with a single output 'i'.\n",
    "@pipefunc(output_name=\"i\")\n",
    "def add_gh(h, g):\n",
    "    return h + g\n",
    "\n",
    "\n",
    "# Create a pipeline with the defined functions and visualize it.\n",
    "pipeline_multiple = Pipeline([add_ab, mul_bc, calc_cde, add_gh])\n",
    "pipeline_multiple.visualize()\n",
    "final_func = pipeline_multiple.func(\"i\")\n",
    "final_func(a=1, b=2, x=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "**(Sneak peak of a later section: simplifying the pipeline)**\n",
    "\n",
    "The pipeline can be simplified by combining `calc_cde` and `add_gh` into a single pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_pipeline_multiple = pipeline_multiple.simplified_pipeline(\"i\")\n",
    "simplified_pipeline_multiple.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Note that, in the simplified pipeline, the full output of `calc_cde` (i.e., `g, h`) is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the full output of calc_cde (g, h) is needed, we can't use the simplified pipeline.\n",
    "out_full = pipeline_multiple.func(\"i\").call_full_output(a=1, b=2, x=3)\n",
    "out_full_red = simplified_pipeline_multiple.func(\"i\").call_full_output(a=1, b=2, x=3)\n",
    "print(f\"Full output of f_e:\\n{out_full}\")\n",
    "print(f\"Full output of f_e after simplification:\\n{out_full_red}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using the `renames` Feature\n",
    "\n",
    "The `renames` attribute in `pipefunc` allows you to rename the inputs and outputs of a function before passing them to the next step in the pipeline.\n",
    "This can be particularly useful when the same function is used multiple times in a pipeline, or when you want to provide more meaningful names to the inputs and outputs.\n",
    "\n",
    "### Example: Renaming Parameters\n",
    "\n",
    "In this example, we demonstrate how to use the `renames` attribute to rename the inputs of a function before they are passed to the next step in the pipeline.\n",
    "\n",
    "> ⚠️ Instead of using the `@pipefunc` decorator (which creates `pipefunc.PipeFunc` object), we will create `PipeFunc` objects directly and specify the `renames` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import PipeFunc, Pipeline\n",
    "\n",
    "\n",
    "def prod(a, b):\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# We're going to use these functions multiple times in the pipeline\n",
    "pipeline_renames = Pipeline(\n",
    "    [\n",
    "        PipeFunc(prod, output_name=\"prod1\"),\n",
    "        PipeFunc(prod, output_name=\"prod2\", renames={\"a\": \"x\", \"b\": \"y\"}),\n",
    "        PipeFunc(subtract, output_name=\"delta1\", renames={\"a\": \"prod1\", \"b\": \"prod2\"}),\n",
    "        PipeFunc(subtract, output_name=\"delta2\", renames={\"a\": \"prod2\", \"b\": \"prod1\"}),\n",
    "        PipeFunc(prod, output_name=\"result\", renames={\"a\": \"delta1\", \"b\": \"delta2\"}),\n",
    "    ],\n",
    ")\n",
    "\n",
    "inputs = {\"a\": 1, \"b\": 2, \"x\": 3, \"y\": 4}\n",
    "results = pipeline_renames(\"result\", **inputs)\n",
    "\n",
    "# Output the results\n",
    "print(\"Results:\", results)\n",
    "\n",
    "pipeline_renames.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "1. **Function Definitions**:\n",
    "\n",
    "   - `prod(a, b)`: Multiples two numbers and returns the result.\n",
    "   - `subtract(a, b)`: Subtracts `b` from `a` and returns the result.\n",
    "\n",
    "2. **Pipeline Construction**:\n",
    "\n",
    "   - **Step 1**: `PipeFunc(prod, output_name=\"prod1\")`\n",
    "     - Multiples inputs `a` and `b` and stores the result as `prod1`.\n",
    "   - **Step 2**: `PipeFunc(prod, output_name=\"prod2\", renames={\"a\": \"x\", \"b\": 'y'})`\n",
    "     - Multiples inputs `x` and `y` and stores the result as `prod2`.\n",
    "   - **Step 3**: `PipeFunc(subtract, output_name=\"delta1\", renames={\"a\": \"prod1\", \"b\": \"prod2\"})`\n",
    "     - Subtracts `prod2` from `prod1` and stores the result as `delta1`.\n",
    "   - **Step 4**: `PipeFunc(subtract, output_name=\"delta2\", renames={\"a\": \"prod2\", \"b\": \"prod1\"})`\n",
    "     - Subtracts `prod1` from `prod2` and stores the result as `delta2`.\n",
    "   - **Step 5**: `PipeFunc(prod, output_name=\"result\", renames={\"a\": \"delta1\", \"b\": \"delta2\"})`\n",
    "     - Multiples `delta1` and `delta2` and stores the result as `result`.\n",
    "\n",
    "3. **Inputs and Execution**:\n",
    "\n",
    "   - The inputs `{\"a\": 1, \"b\": 2, \"x\": 3, \"y\": 4}` are provided to the pipeline.\n",
    "   - The pipeline executes in the defined order, renaming inputs and outputs as specified.\n",
    "   - The final result is computed and returned as `result`.\n",
    "\n",
    "4. **Output**:\n",
    "   - The final output of the pipeline is printed, showing the computed `result`.\n",
    "\n",
    "By using the `renames` attribute, this example demonstrates how to manage and distinguish between different uses of the same function within a pipeline, ensuring the correct values are processed at each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "One can also apply the renames afterwards using the `update_renames` method. Or even to the entire pipeline, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_renames2 = pipeline_renames.copy()\n",
    "pipeline_renames2.update_renames(\n",
    "    {\n",
    "        \"a\": \"aa\",\n",
    "        \"b\": \"bb\",\n",
    "        \"x\": \"xx\",\n",
    "        \"y\": \"yy\",\n",
    "        \"result\": \"final_result\",  # Rename the `output_name` of the last function\n",
    "    },\n",
    "    update_from=\"current\",  # update from the current renames, not the original\n",
    ")\n",
    "pipeline_renames2(aa=1, bb=2, xx=3, yy=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Also check out these `Pipeline` methods:\n",
    "\n",
    "- `Pipeline.update_defaults`\n",
    "- `Pipeline.update_bound`\n",
    "\n",
    "and these `PipeFunc` methods:\n",
    "\n",
    "- `PipeFunc.update_renames`\n",
    "- `PipeFunc.update_defaults`\n",
    "- `PipeFunc.update_bound`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Combine pipelines\n",
    "\n",
    "Different pipelines can be combined into a single pipeline using the `Pipeline.combine` method or the `|` operator.\n",
    "\n",
    "In cases the output names and arugments do not match up, we can rename the parameters of an entire pipeline using the `update_renames` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"c\")\n",
    "def f(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"d\")\n",
    "def g(b, c, x=1):\n",
    "    return b + c + x\n",
    "\n",
    "\n",
    "pl1 = Pipeline([f, g])\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"e\")\n",
    "def h(cc, dd, xx=2):\n",
    "    return cc + dd + xx\n",
    "\n",
    "\n",
    "pl2 = Pipeline([h])\n",
    "\n",
    "# We now have two pipelines, `pl1` and `pl2`, that we want to combine\n",
    "# into a single pipeline. However, they have different inputs and defaults.\n",
    "# Let's update the renames and defaults of `pl2` to match `pl1`.\n",
    "pl2_ = pl2.copy()\n",
    "pl2_.update_renames({\"cc\": \"c\", \"dd\": \"d\", \"xx\": \"x\"})\n",
    "pl2_.update_defaults({\"x\": 1})\n",
    "combined_pipeline = pl1 | pl2_  # or use `pl1.combine(pl2_)`\n",
    "\n",
    "combined_pipeline.visualize()\n",
    "\n",
    "# The combined pipeline can now be used as a single pipeline\n",
    "result = combined_pipeline(a=2, b=3, x=2)\n",
    "print(result)  # Output: 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Just to see another quick example of combining pipelines (even though it makes no sense to combine these pipelines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_silly = pipeline | pipeline_multiple | pipeline_renames | combined_pipeline\n",
    "pipeline_silly.visualize()\n",
    "# e.g., if we want to get the output of `g` in the `pipeline` (not the leaf node!):\n",
    "pipeline_silly(\"g\", a=1, b=2, x=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Simplifying Pipelines\n",
    "\n",
    "Consider the following pipeline (look at the `visualize()` output to see the structure of the pipeline):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline\n",
    "\n",
    "\n",
    "def f1(a, b, c, d):\n",
    "    return a + b + c + d\n",
    "\n",
    "\n",
    "def f2(a, b, e):\n",
    "    return a + b + e\n",
    "\n",
    "\n",
    "def f3(a, b, f1):\n",
    "    return a + b + f1\n",
    "\n",
    "\n",
    "def f4(f1, f3):\n",
    "    return f1 + f3\n",
    "\n",
    "\n",
    "def f5(f1, f4):\n",
    "    return f1 + f4\n",
    "\n",
    "\n",
    "def f6(b, f5):\n",
    "    return b + f5\n",
    "\n",
    "\n",
    "def f7(a, f2, f6):\n",
    "    return a + f2 + f6\n",
    "\n",
    "\n",
    "# If the functions are not decorated with @pipefunc,\n",
    "# they will be wrapped and the output_name will be the function name\n",
    "pipeline_complex = Pipeline([f1, f2, f3, f4, f5, f6, f7])\n",
    "pipeline_complex(\"f7\", a=1, b=2, c=3, d=4, e=5)\n",
    "pipeline_complex.visualize(color_combinable=True)  # combinable functions have the same color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "In the example code above, the complex pipeline composed of multiple functions (`f1`, `f2`, `f3`, `f4`, `f5`, `f6`, `f7`) can be simplified by merging the nodes `f1`, `f3`, `f4`, `f5`, `f6` into a single node.\n",
    "This merging process simplifies the pipeline and allows to reduce the number of functions that need to be cached/saved.\n",
    "\n",
    "The method `reduced_pipeline` from the `Pipeline` class is used to generate this simplified version of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_pipeline_complex = pipeline_complex.simplified_pipeline(\"f7\")\n",
    "simplified_pipeline_complex.visualize()  # A `NestedPipeFunc` will have a red edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "However, simplifying a pipeline comes with a trade-off. The simplification process removes intermediate nodes that may be necessary for debugging or inspection.\n",
    "\n",
    "For instance, if a developer wants to monitor the output of `f3` while processing the pipeline, they would not be able to do so in the simplified pipeline as `f3` has been merged into a single node. Hence, while a simplified pipeline can speed up the computation, it may limit the ability to examine intermediate computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "The simplified pipeline now contains a `NestedPipeFunc` object, which is a subclass of `PipeFunc` but contains an internal pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_pipeline_complex.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_func = simplified_pipeline_complex.functions[-1]\n",
    "print(f\"{nested_func.parameters=}, {nested_func.output_name=}, {nested_func(a=1, b=2, c=3, d=4)=}\")\n",
    "nested_func.pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Another graph simplification example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"d\", \"e\"))\n",
    "def calc_de(b, g, x=1):\n",
    "    pass\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"g\", \"h\"))\n",
    "def calc_gh(a, x=1):\n",
    "    pass\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"gg\")\n",
    "def calc_gg(g):\n",
    "    pass\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"i\")\n",
    "def calc_i(gg, b, e):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create a pipeline with the defined functions and visualize it\n",
    "pipe3 = Pipeline([calc_de, calc_gh, calc_i, calc_gg])\n",
    "pipe3.visualize(color_combinable=True)\n",
    "pipe3.simplified_pipeline(\"i\").visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "We can also manually nest functions in the pipeline by either creating a `NestedPipeFunc` object or using the `pipeline.nest_funcs` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3_manual = pipe3.copy()\n",
    "# combine the nodes manually\n",
    "nested_func = pipe3_manual.nest_funcs({\"gg\", \"i\", \"d\"})  # returns the new `NestedPipeFunc`\n",
    "pipe3_manual.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned `NestedPipeFunc` object contains an internal pipeline\n",
    "nested_func.pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Working with Resources Report\n",
    "\n",
    "The `print_profiling_stats()` method of the `pipeline` provides useful information on the performance of the functions in the pipeline such as CPU usage, memory usage, average time, and the number of times each function was called. This feature is only available if `profile=True` when creating the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print the number of times each function was called\n",
    "# CPU, memory, and time usage is also reported\n",
    "pipeline.print_profiling_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "This report can be beneficial in performance tuning and identifying bottlenecks in your pipeline. You can identify which functions are consuming the most resources and adjust your pipeline accordingly.\n",
    "\n",
    "You can also look all the stats directly with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.profiling_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running Map-Reduce Pipelines using `MapSpec`\n",
    "\n",
    "This section shows how `pipefunc` uses `mapspec` to automate data distribution across Map-Reduce tasks.\n",
    "\n",
    "> **Note:** ⚠️ The mapping computation of the pipeline is done in parallel using the `concurrent.futures.ProcessPoolExecutor` whenever `pipeline.map(..., parallel=True)` (default).\n",
    "\n",
    "### Example: Simple reduction pipeline\n",
    "\n",
    "The script below demonstrates a two-step pipeline: doubling each integer in an input list, followed by summing all the doubled values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.map import load_outputs, load_xarray_dataset\n",
    "from pipefunc.typing import Array\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"y\", mapspec=\"x[i] -> y[i]\")\n",
    "def double_it(x: int) -> int:\n",
    "    assert isinstance(x, int)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"sum\")\n",
    "def take_sum(y: Array[int]) -> int:\n",
    "    assert isinstance(y, np.ndarray)\n",
    "    return sum(y)\n",
    "\n",
    "\n",
    "pipeline_map = Pipeline([double_it, take_sum])\n",
    "\n",
    "inputs = {\"x\": [0, 1, 2, 3]}\n",
    "run_folder = \"my_run_folder\"\n",
    "results = pipeline_map.map(inputs, run_folder=run_folder)\n",
    "\n",
    "# Check the results in the resulting dict\n",
    "assert results[\"y\"].output.tolist() == [0, 2, 4, 6]\n",
    "assert results[\"sum\"].output == 12\n",
    "\n",
    "# Or load the outputs from disk\n",
    "assert load_outputs(\"y\", run_folder=run_folder).tolist() == [0, 2, 4, 6]\n",
    "assert load_outputs(\"sum\", run_folder=run_folder) == 12\n",
    "\n",
    "# Or also load from disk but as an `xarray.Dataset:\n",
    "load_xarray_dataset(run_folder=run_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "1. **Using `mapspec` for Data Distribution**:\n",
    "\n",
    "   - The `mapspec` attribute in the `@pipefunc` decorator defines how data is distributed across computations. In `double_it`, `mapspec=\"x[i] -> y[i]\"` specifies that each element `i` of the input array `x` is independently processed to produce the corresponding element `i` in the output array `y`. Because `take_sum` does not have a `mapspec`, it receives the entire array `y` for aggregation.\n",
    "\n",
    "2. **Function Definitions**:\n",
    "\n",
    "   - `double_it`: Doubles each integer, demonstrating a simple stateless operation that can be easily parallelized.\n",
    "   - `take_sum`: Aggregates all elements of the resulting array into a single sum, serving as the reduce step in this Map-Reduce example.\n",
    "\n",
    "3. **Pipeline Execution**:\n",
    "\n",
    "   - The `Pipeline.map` method executes the pipeline with specific inputs and a directory for temporary run files, showcasing how PipeFunc manages data flow and execution state across the pipeline. Alternatively, use the `pipefunc.map.run` function.\n",
    "\n",
    "4. **Results Verification**:\n",
    "\n",
    "   - Assertions check that the final sum of doubled numbers is correct, ensuring both the integrity and correctness of the pipeline's execution.\n",
    "\n",
    "5. **Output Retrieval**:\n",
    "   - The `load_outputs` function demonstrates how to retrieve and verify results post-computation, confirming the output is stored and accessible as expected.\n",
    "\n",
    "### Example: Cross-product of inputs and multidiagonal aggregation\n",
    "\n",
    "This example shows how to compute the outer product of two input vectors (`x` and `y`) and then aggregate the resulting matrix along rows, and finally reduce the computation to a single `float` by taking the `norm` of the resulting `aggregated` vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"z\", mapspec=\"x[i], y[j] -> z[i, j]\")\n",
    "def multiply_elements(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"aggregated\", mapspec=\"z[i, :] -> aggregated[i]\")\n",
    "def aggregate_rows(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Sum the elements of each row in matrix z.\"\"\"\n",
    "    return np.sum(z)\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"norm\")\n",
    "def compute_norm(aggregated: np.ndarray) -> float:\n",
    "    \"\"\"Compute the Euclidean norm of the vector aggregated.\"\"\"\n",
    "    return np.linalg.norm(aggregated)\n",
    "\n",
    "\n",
    "pipeline_norm = Pipeline([multiply_elements, aggregate_rows, compute_norm])\n",
    "inputs = {\"x\": [1, 2, 3], \"y\": [4, 5, 6]}\n",
    "results = pipeline_norm.map(inputs, run_folder=\"my_run_folder\")\n",
    "print(\"Norm of the aggregated sums:\", results[\"norm\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "1. **Matrix Creation (`multiply_elements`)**:\n",
    "\n",
    "   - Each combination of elements from arrays `x` and `y` is multiplied to form the matrix `z`. The `mapspec` `\"x[i], y[j] -> z[i, j]\"` ensures that every pair of elements is processed to generate a 2D matrix.\n",
    "\n",
    "2. **Row Aggregation (`aggregate_rows`)**:\n",
    "\n",
    "   - The matrix `z` is then processed row by row to sum the values, creating an aggregated result for each row. The `mapspec` `\"z[i, :] -> aggregated[i]\"` directs the pipeline to apply the summation across each row, transforming a 2D array into a 1D array of row sums.\n",
    "\n",
    "3. **Vector Norm Calculation (`compute_norm`)**:\n",
    "   - Finally, the norm of the aggregated vector is computed, providing a single scalar value that quantifies the magnitude of the vector formed from row sums. This step does not require a `mapspec` as it takes the entire output from the previous step and produces a single output.\n",
    "\n",
    "### Example: Handling Dynamic Output Shapes\n",
    "\n",
    "It is not always possible to predict output shapes directly from the input parameters. For instance, when a function generates a list whose length depends on the runtime value of an input, PipeFunc cannot automatically determine the output dimensions. To manage such cases, developers must manually specify these output shapes.\n",
    "\n",
    "We also show an alternative way of specifying the `mapspec` when creating the `Pipeline` object by passing `tuple`s to the `Pipeline` constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.typing import Array\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"x\")\n",
    "def generate_ints(n: int) -> list[int]:\n",
    "    \"\"\"Generate a list of integers from 0 to n-1.\"\"\"\n",
    "    return list(range(n))\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"y\")\n",
    "def double_it(x: int) -> int:\n",
    "    \"\"\"Double the input integer.\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"sum\")\n",
    "def take_sum(y: Array[int]) -> int:\n",
    "    \"\"\"Sum a list of integers.\"\"\"\n",
    "    return sum(y)\n",
    "\n",
    "\n",
    "# One is able to specify the mapspec when creating the Pipeline object\n",
    "# by passing a tuple of the function and the mapspec string.\n",
    "pipeline_sum = Pipeline(\n",
    "    [\n",
    "        generate_ints,\n",
    "        (double_it, \"x[i] -> y[i]\"),  # Apply doubling to each element in the list\n",
    "        take_sum,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "inputs = {\"n\": 4}\n",
    "internal_shapes = {\"x\": (4,)}  # Explicitly specify the shape of 'x' output from generate_ints\n",
    "\n",
    "# Run the pipeline\n",
    "results = pipeline_sum.map(inputs, internal_shapes=internal_shapes, run_folder=\"my_run_folder\")\n",
    "print(\"Sum of doubled integers:\", results[\"sum\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "This example demonstrates explicitly specifying output shapes via `internal_shapes` in scenarios where the output size is dynamically determined by input values.\n",
    "\n",
    "### Example: Zipped Inputs and Pairwise Computation\n",
    "\n",
    "This pipeline processes zipped inputs `x` and `y` with independent `z` to compute a function across all combinations, producing a 2D matrix `r`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"r\")\n",
    "def process_elements(x: int, y: int, z: int) -> float:\n",
    "    return x * y + z\n",
    "\n",
    "\n",
    "pipeline_proc = Pipeline([(process_elements, \"x[a], y[a], z[b] -> r[a, b]\")])\n",
    "\n",
    "inputs = {\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"z\": [7, 8]}\n",
    "\n",
    "results = pipeline_proc.map(inputs, run_folder=\"my_run_folder\")\n",
    "output_matrix = results[\"r\"].output\n",
    "print(\"Output Matrix:\\n\", output_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "- **Function `process_elements`**:\n",
    "\n",
    "  - Takes three inputs: `x`, `y`, and `z`. For each pair `(x[a], y[a])`, the function is applied with each `z[b]`.\n",
    "\n",
    "- **Pipeline Definition**:\n",
    "\n",
    "  - The `mapspec` `\"x[a], y[a], z[b] -> r[a, b]\"` specifies how elements from the inputs are to be combined. It states that each element from the paired inputs `x` and `y` (indexed by `a`) should be processed with each element from `z` (indexed by `b`), resulting in a 2D output array `r`.\n",
    "\n",
    "- **Outputs**:\n",
    "  - The output `r` is a 2-dimensional matrix where the dimensions are determined by the lengths of `x`/`y` and `z`. Each element of this matrix represents the computation result for a specific combination of inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Example: Physics based example\n",
    "\n",
    "This example demonstrates using the `pipefunc` for a physics-based simulation. The goal is to create a pipeline for geometry creation, meshing, material assignment, and electrostatics calculations, culminating in computing the average charge.\n",
    "\n",
    "> Note: this example is based on the [`aiida-dynamic-workflows` tutorial](https://github.com/microsoft/aiida-dynamic-workflows/blob/4d452ed3be4192dc5b2c8f40690f82c3afcaa7a8/examples/02-workflows.md).\n",
    "\n",
    "We start with defining a few `dataclasses` to represent the geometry, mesh, and material properties. We then define functions to create the geometry, mesh it, assign materials, and calculate the electrostatics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Geometry:\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Mesh:\n",
    "    geometry: Geometry\n",
    "    mesh_size: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Materials:\n",
    "    geometry: Geometry\n",
    "    materials: list[str]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Electrostatics:\n",
    "    mesh: Mesh\n",
    "    materials: Materials\n",
    "    voltages: list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.map import load_outputs\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"geo\")\n",
    "def make_geometry(x: float, y: float) -> Geometry:\n",
    "    return Geometry(x, y)\n",
    "\n",
    "\n",
    "@pipefunc(output_name=(\"mesh\", \"coarse_mesh\"))\n",
    "def make_mesh(\n",
    "    geo: Geometry,\n",
    "    mesh_size: float,\n",
    "    coarse_mesh_size: float,\n",
    ") -> tuple[Mesh, Mesh]:\n",
    "    return Mesh(geo, mesh_size), Mesh(geo, coarse_mesh_size)\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"materials\")\n",
    "def make_materials(geo: Geometry) -> Materials:\n",
    "    return Materials(geo, [\"i\", \"j\", \"c\"])\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"electrostatics\")\n",
    "def run_electrostatics(\n",
    "    mesh: Mesh,\n",
    "    materials: Materials,\n",
    "    V_left: float,  # noqa: N803\n",
    "    V_right: float,  # noqa: N803\n",
    ") -> Electrostatics:\n",
    "    return Electrostatics(mesh, materials, [V_left, V_right])\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"charge\")\n",
    "def get_charge(electrostatics: Electrostatics) -> float:\n",
    "    # obviously not actually the charge; but we should return _some_ number that\n",
    "    # is \"derived\" from the electrostatics.\n",
    "    return sum(electrostatics.voltages)\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"average_charge\")\n",
    "def average_charge(charge: np.ndarray) -> float:\n",
    "    return np.mean(charge)\n",
    "\n",
    "\n",
    "pipeline_charge = Pipeline(\n",
    "    [\n",
    "        make_geometry,\n",
    "        make_mesh,\n",
    "        make_materials,\n",
    "        (run_electrostatics, \"V_left[i], V_right[j] -> electrostatics[i, j]\"),\n",
    "        (get_charge, \"electrostatics[i, j] -> charge[i, j]\"),\n",
    "        average_charge,  # this function receives the full 2D array of charges!\n",
    "    ],\n",
    ")\n",
    "pipeline_charge.visualize(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Let's run the map for some inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"V_left\": np.linspace(0, 2, 3),\n",
    "    \"V_right\": np.linspace(-0.5, 0.5, 2),\n",
    "    \"x\": 0.1,\n",
    "    \"y\": 0.2,\n",
    "    \"mesh_size\": 0.01,\n",
    "    \"coarse_mesh_size\": 0.05,\n",
    "}\n",
    "\n",
    "run_folder = \"my_run_folder\"\n",
    "results = pipeline_charge.map(inputs, run_folder=run_folder, parallel=False)\n",
    "assert results[\"average_charge\"].output == 1.0\n",
    "assert results[\"average_charge\"].output_name == \"average_charge\"\n",
    "assert load_outputs(\"average_charge\", run_folder=run_folder) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "This example highlighted how to run a simulation that included a map-reduce operation.\n",
    "Often we want to sweep this over multiple parameters.\n",
    "You could add all the `mapspec`s required to map an additional parameter.\n",
    "Alternatively, you can use the `pipeline.add_mapspec_axis` method to add an axis to parameters of the pipeline.\n",
    "\n",
    "See the example below, where we extend the `mapspec`s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a cross-product of x and y\n",
    "pipeline_charge.add_mapspec_axis(\"x\", axis=\"a\")\n",
    "pipeline_charge.add_mapspec_axis(\"y\", axis=\"b\")\n",
    "\n",
    "# And also a cross-product of the zipped mesh_size and coarse_mesh_size\n",
    "pipeline_charge.add_mapspec_axis(\"mesh_size\", axis=\"c\")\n",
    "pipeline_charge.add_mapspec_axis(\"coarse_mesh_size\", axis=\"c\")\n",
    "\n",
    "# Finally, the mapspecs become, which shows a 3D array for the `average_charge`:\n",
    "pipeline_charge.mapspecs_as_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_charge.visualize(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "Let's run it on a 2x2x2 grid of inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"V_left\": np.linspace(0, 2, 3),\n",
    "    \"V_right\": np.linspace(-0.5, 0.5, 2),\n",
    "    \"x\": np.linspace(0.1, 0.2, 2),\n",
    "    \"y\": np.linspace(0.2, 0.3, 2),\n",
    "    \"mesh_size\": [0.01, 0.02],\n",
    "    \"coarse_mesh_size\": [0.05, 0.06],\n",
    "}\n",
    "results = pipeline_charge.map(inputs, run_folder=run_folder, parallel=False)\n",
    "output = results[\"average_charge\"].output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "We can also load all data as `xarray.Dataset`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc.map import load_xarray_dataset\n",
    "\n",
    "ds = load_xarray_dataset(run_folder=run_folder)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "Or specify the `output_name` to load only specific outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_charge = load_xarray_dataset(\"average_charge\", run_folder=run_folder)\n",
    "avg_charge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "Now imagine that the electrostatics object is a very large object that we cannot afford to save and load from disk.\n",
    "For this purpose there is the `pipfunc.NestedPipeFunc` class that allows to combine multiple functions into a single function. We can then tell it to not return the output of the intermediate functions by specifying which outputs to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_charge2 = pipeline_charge.copy()\n",
    "nested_func = pipeline_charge2.nest_funcs(\n",
    "    {\"electrostatics\", \"charge\"},\n",
    "    new_output_name=\"charge\",\n",
    "    # We can also specify `(\"charge\", \"electrostatics\")` to get both outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "This `nested_func` contains an internal pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_func.pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "When visualizing the pipeline, you can see that the `NestedFunc` is shown as a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_charge2.visualize(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "### Custom parallelism\n",
    "\n",
    "By default when `pipeline.map(..., parallel=True)` is used, the pipeline is executed in parallel using the `concurrent.futures.ProcessPoolExecutor`. However, you can also specify a custom executor to control the parallelism of the pipeline execution.\n",
    "\n",
    "It works with any custom executor that has the `concurrent.futures.Executor` interface, so for example it works with:\n",
    "\n",
    "- `concurrent.futures.ProcessPoolExecutor`\n",
    "- `concurrent.futures.ThreadPoolExecutor`\n",
    "- `ipyparallel.Client().executor()`\n",
    "- `dask.distributed.Client().get_executor()`\n",
    "- `mpi4py.futures.MPIPoolExecutor()`\n",
    "- `loky.get_reusable_executor()`\n",
    "\n",
    "To just change the number of cores while using the default executor, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"double\", mapspec=\"x[i] -> double[i]\")\n",
    "def double_it(x: int) -> int:\n",
    "    print(f\"{datetime.datetime.now()} - Running double_it for x={x}\")\n",
    "    time.sleep(1)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"half\", mapspec=\"x[i] -> half[i]\")\n",
    "def half_it(x: int) -> int:\n",
    "    print(f\"{datetime.datetime.now()} - Running half_it for x={x}\")\n",
    "    time.sleep(1)\n",
    "    return x // 2\n",
    "\n",
    "\n",
    "@pipefunc(output_name=\"sum\")\n",
    "def take_sum(half: np.ndarray, double: np.ndarray) -> int:\n",
    "    print(f\"{datetime.datetime.now()} - Running take_sum\")\n",
    "    return sum(half + double)\n",
    "\n",
    "\n",
    "pipeline_parallel = Pipeline([double_it, half_it, take_sum])\n",
    "inputs = {\"x\": [0, 1, 2, 3]}\n",
    "run_folder = \"my_run_folder\"\n",
    "executor = ProcessPoolExecutor(max_workers=8)  # use 8 processes\n",
    "results = pipeline_parallel.map(\n",
    "    inputs,\n",
    "    run_folder=run_folder,\n",
    "    parallel=True,\n",
    "    executor=executor,\n",
    "    storage=\"shared_memory_dict\",\n",
    ")\n",
    "print(results[\"sum\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "> ⚠️ In this pipeline, `double_it` and `half_it` are doubly parallel; both the map is parallel and the two functions are executed at the same time, note the timestamps and the `sleep()` calls.\n",
    "> See the `visualize()` output to see the structure of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_parallel.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Advanced: [Adaptive Scheduler](https://adaptive-scheduler.readthedocs.io/) integration\n",
    "\n",
    "PipeFunc can also be used with the `adaptive_scheduler` package to run the pipeline on a cluster. This allows you to run the pipeline on a cluster (e.g., with SLURM) without having to worry about the details of submitting jobs and managing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pipefunc import Pipeline, pipefunc\n",
    "from pipefunc.map.adaptive import create_learners\n",
    "from pipefunc.resources import Resources\n",
    "\n",
    "\n",
    "# Pass in a `Resources` object that specifies the resources needed for each function\n",
    "@pipefunc(output_name=\"double\", mapspec=\"x[i] -> double[i]\", resources=Resources(cpus=5))\n",
    "def double_it(x: int) -> int:\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "# Or specify the resources as a dictionary\n",
    "@pipefunc(output_name=\"half\", mapspec=\"x[i] -> half[i]\", resources={\"memory\": \"8GB\"})\n",
    "def half_it(x: int) -> int:\n",
    "    return x // 2\n",
    "\n",
    "\n",
    "# Specify delayed resources that are used inside the function; \"internal\" parallelization\n",
    "@pipefunc(\n",
    "    output_name=\"sum\",\n",
    "    resources=lambda kw: {\"cpus\": len(kw[\"half\"]), \"parallelization_mode\": \"internal\"},\n",
    "    resources_variable=\"resources\",\n",
    ")\n",
    "def take_sum(half: np.ndarray, double: np.ndarray, resources: Resources) -> int:\n",
    "    with ProcessPoolExecutor(resources.cpus) as executor:\n",
    "        # Do some printing in parallel (not smart, but just to show the parallelization)\n",
    "        list(executor.map(print, range(resources.cpus)))\n",
    "    return sum(half + double)\n",
    "\n",
    "\n",
    "pipeline_adapt = Pipeline([double_it, half_it, take_sum])\n",
    "\n",
    "inputs = {\"x\": [0, 1, 2, 3]}\n",
    "run_folder = \"my_run_folder\"\n",
    "learners_dict = create_learners(\n",
    "    pipeline_adapt,\n",
    "    inputs,\n",
    "    run_folder=run_folder,\n",
    "    split_independent_axes=True,  # Split up into as many independent jobs as possible\n",
    ")\n",
    "kwargs = learners_dict.to_slurm_run(\n",
    "    returns=\"kwargs\",  # or \"run_manager\" to return a `adaptive_scheduler.RunManager` object\n",
    "    default_resources={\"cpus\": 2, \"memory\": \"8GB\"},\n",
    ")\n",
    "kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parallel Execution and Caching\n",
    "\n",
    "To enable parallel execution, you can use Python's built-in `concurrent.futures.ProcessPoolExecutor`. To enable caching, simply set the `cache` attribute to `True` for each function. This can be useful to avoid recomputing results when calling the same function with the same arguments multiple times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "for f in pipeline.functions:\n",
    "    # Enable caching for all functions\n",
    "    # See next section to only cache based on a certain parameter sweep\n",
    "    f.cache = True\n",
    "\n",
    "pf_e = pipeline.func(\"e\")\n",
    "sequence = 10 * [{\"a\": 2, \"b\": 3, \"x\": 1}]\n",
    "with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    results = executor.map(pf_e.call_with_dict, sequence)\n",
    "    print(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "The cache is populated _**even when using parallel execution**_. To see the cache, you can use the `cache` attribute on the pipeline.\n",
    "\n",
    "The keys of the cache are always in terms of the root arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cache object: {pipeline.cache}\")\n",
    "pipeline.cache.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parameter Sweeps\n",
    "\n",
    "Parameter sweeps are a technique used in computational simulations to explore the parameter space of a model or system.\n",
    "\n",
    "In the provided example, the `generate_sweep` method is used to generate a set of combinations of input parameters `a`, `b`, `c`, `d`, and `e` for the function `f7`.\n",
    "The `generate_sweep` method takes a dictionary of parameters as input and returns a list of dictionaries, where each dictionary represents a combination of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc.sweep import Sweep\n",
    "\n",
    "combos = {\n",
    "    \"a\": [0, 1, 2],\n",
    "    \"b\": [0, 1, 2],\n",
    "    \"c\": [0, 1, 2],\n",
    "    \"d\": [0, 1, 2],\n",
    "    \"e\": [0, 1, 2],\n",
    "}\n",
    "# This means a Cartesian product of all the values in the lists\n",
    "# while zipping (\"a\", \"b\").\n",
    "sweep = Sweep(combos, dims=[(\"a\", \"b\"), \"c\", \"d\", \"e\"])\n",
    "sweep.list()[:10]  # show the first 10 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "The function `set_cache_for_sweep` then enables caching for nodes in the pipeline that are expected to be executed two or more times during the parameter sweep. Caching improves the efficiency of the sweep by storing and reusing results of repeated computations, rather than performing the same computation multiple times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunc.sweep import set_cache_for_sweep\n",
    "\n",
    "set_cache_for_sweep(\"f7\", simplified_pipeline_complex, sweep, min_executions=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "We can now run the sweep using e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    simplified_pipeline_complex.run(\"f7\", kwargs=combo, full_output=True) for combo in sweep.list()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
