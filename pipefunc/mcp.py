"""Autogenerated MCP server for Pipefunc pipelines."""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, Literal

from pipefunc._pipeline._autodoc import PipelineDocumentation, format_pipeline_docs
from pipefunc._utils import requires

if TYPE_CHECKING:
    import fastmcp

    from pipefunc._pipeline._base import Pipeline

logger = logging.getLogger(__name__)

_PIPEFUNC_INSTRUCTIONS = """\
This MCP server executes Pipefunc computational pipelines.
Pipefunc creates function pipelines as DAGs where functions are automatically connected based on input/output dependencies.

CORE CONCEPTS:
- Pipeline: Sequence of interconnected functions forming a computational workflow
- MapSpec: String syntax defining how arrays map between functions
- Parallel Execution: Automatically parallelizes computations across parameter combinations
- Parameter Sweeps: Process multiple input combinations efficiently

EXECUTION PARAMETERS:
- input: Dictionary with parameter values (single values or arrays)
- parallel: Boolean (default true) - enables parallel execution
- run_folder: Optional string - directory to save intermediate results

INPUT FORMATS:
1. Single values: {{"param1": 5, "param2": 10}} - executes once with these values
2. Array sweeps: {{"param1": [1,2,3], "param2": [4,5]}} - creates parameter combinations based on mapspec
3. Mixed: {{"data": [1,2,3], "constant": 10}} - arrays are swept, single values used for all iterations

MAPSPEC SYNTAX:
- "x[i] -> y[i]": Element-wise processing (same array length)
- "a[i], b[j] -> result[i,j]": Cross-product (all combinations)
- "x[i], y[i] -> z[i]": Zipped processing (paired elements)
- "x[i,:] -> y[i]": Reduction across dimension
- "... -> x[i]": Dynamic array generation

INDEX RULES:
- Same index letter ([i], [i]): Elements processed together (zipped)
- Different indices ([i], [j]): Create cross-product combinations
- No indices: Single values used for all iterations

OUTPUT FORMAT:
Returns dictionary with all pipeline outputs. Each output contains:
- "output": Computed result (converted to JSON-compatible format)
- "shape": Array dimensions (if applicable)

EXECUTION MODES:
- parallel=true: Functions execute concurrently, faster for large sweeps
- parallel=false: Sequential execution, more reliable for custom objects, better for debugging

COMMON ISSUES:
- Shape mismatches: Verify mapspec definitions match expected array dimensions
- Type errors: Ensure input types match function parameter types

PIPELINE DESCRIPTION:
{pipeline_description}
"""

_PIPELINE_DESCRIPTION_TEMPLATE = """\
Execute the pipeline with input values. This method works for both single values and arrays/lists.

PIPELINE INFORMATION:
{pipeline_info}

MAPSPEC DEFINITIONS:
{mapspec_section}

INPUT FORMAT:
{input_format}

DETAILED PIPELINE DOCUMENTATION:
{documentation}
"""

_NO_MAPSPEC_INPUT_FORMAT = """\
Single values only:
  {"a": 5, "b": 10, "x": 2}
  → Each parameter gets a single value

This will execute the pipeline once with these specific values and return the result.
"""

_MAPSPEC_INPUT_FORMAT = """\

1. Simple element-wise mapping:
   {"x": [1, 2, 3, 4]}
   → If function has mapspec "x[i] -> y[i]", this will process each x value independently

2. Cross-product of inputs:
   {"a": [1, 2], "b": [10, 20]}
   → If functions have mapspecs like "a[i], b[j] -> result[i, j]", this creates all combinations

3. Zipped inputs (same index):
   {"x": [1, 2, 3], "y": [4, 5, 6]}
   → If function has mapspec "x[i], y[i] -> z[i]", this pairs x[0] with y[0], x[1] with y[1], etc.

4. Mixed single values and arrays:
   {"data": [1, 2, 3, 4], "multiplier": 10}
   → Arrays are mapped over, single values are used for all iterations

Key concepts:
- mapspec defines how inputs map to outputs (e.g., "x[i] -> y[i]" means element-wise)
- Arrays with same index letter (like [i]) are processed together
- Arrays with different indices (like [i] and [j]) create cross-products
- Single values work regardless of mapspecs
"""


def _get_pipeline_documentation(pipeline: Pipeline) -> str:
    """Generate formatted pipeline documentation tables using Rich."""
    requires("rich", "griffe", reason="mcp", extras="autodoc")
    from rich.console import Console

    doc = PipelineDocumentation.from_pipeline(pipeline)
    tables = format_pipeline_docs(doc, print_table=False)
    assert tables is not None

    console = Console(no_color=True)
    with console.capture() as capture:
        for table in tables:
            console.print(table, "\n")
    return capture.get()


def _get_pipeline_info_summary(pipeline_name: str, pipeline: Pipeline) -> str:
    """Generate a summary of pipeline information."""
    info = pipeline.info()
    assert info is not None

    def _format(key: str) -> str:
        return ", ".join(info[key]) if info[key] else "None"

    lines = [
        f"Pipeline Name: {pipeline_name}",
        f"Required Inputs: {_format('required_inputs')}",
        f"Optional Inputs: {_format('optional_inputs')}",
        f"Outputs: {_format('outputs')}",
        f"Intermediate Outputs: {_format('intermediate_outputs')}",
    ]
    return "\n".join(lines)


def _get_mapspec_section(pipeline: Pipeline) -> str:
    """Generate mapspec information section."""
    mapspecs = pipeline.mapspecs_as_strings
    if not mapspecs:
        return "None (This pipeline processes single values only)"

    lines = [
        "The following mapspecs define how arrays are processed:",
    ]

    for i, mapspec in enumerate(mapspecs, 1):
        lines.append(f"  {i}. {mapspec}")

    lines.extend(
        [
            "",
            "Mapspec Legend:",
            "- Parameters with [i], [j], etc. represent array dimensions",
            "- Same index letter (e.g., [i]) means elements are processed together (zipped)",
            "- Different indices (e.g., [i] and [j]) create cross-products",
            "- Parameters without indices are used as single values for all iterations",
        ],
    )

    return "\n".join(lines)


def _get_input_format_section(pipeline: Pipeline) -> str:
    """Generate input format examples section."""
    mapspecs = pipeline.mapspecs_as_strings
    return _MAPSPEC_INPUT_FORMAT if mapspecs else _NO_MAPSPEC_INPUT_FORMAT


def _format_tool_description(
    pipeline_info: str,
    mapspec_section: str,
    input_format: str,
    documentation: str,
) -> str:
    """Format a complete tool description using the template."""
    return _PIPELINE_DESCRIPTION_TEMPLATE.format(
        pipeline_info=pipeline_info,
        mapspec_section=mapspec_section,
        input_format=input_format,
        documentation=documentation,
    )


def build_mcp_server(
    pipeline: Pipeline,
    version: str = "1.0.0",
) -> fastmcp.FastMCP:
    """Build the MCP server for the pipeline."""
    requires("mcp", "rich", "griffe", reason="mcp", extras="mcp")
    from fastmcp import FastMCP
    from fastmcp.utilities.types import get_cached_typeadapter

    # Generate all pipeline information sections
    pipeline_name = pipeline.name or "Unnamed Pipeline"
    documentation = _get_pipeline_documentation(pipeline)
    pipeline_info = _get_pipeline_info_summary(pipeline_name, pipeline)
    mapspec_section = _get_mapspec_section(pipeline)
    input_format = _get_input_format_section(pipeline)

    # Format description using the template
    description = _format_tool_description(
        pipeline_info=pipeline_info,
        mapspec_section=mapspec_section,
        input_format=input_format,
        documentation=documentation,
    )

    Model = pipeline.pydantic_model()  # noqa: N806
    Model.model_rebuild()  # Ensure all type references are resolved
    mcp = FastMCP(
        name=pipeline_name,
        version=version,
        instructions=_PIPEFUNC_INSTRUCTIONS.format(
            pipeline_description=pipeline.description or "No description provided.",
        ),
    )

    def execute_pipeline(
        input: Model,  # type: ignore[valid-type] # noqa: A002
        parallel: bool = True,  # noqa: FBT001, FBT002
        run_folder: str | None = None,
    ) -> str:
        """Execute pipeline with input values (works for both single values and arrays)."""
        logger.info(f"Executing pipeline {pipeline.name=} with input: {input}")  # noqa: G004
        result = pipeline.map(
            inputs=input,
            parallel=parallel,
            run_folder=run_folder,
        )
        logger.info(f"Pipeline {pipeline.name=} executed")  # noqa: G004
        # Convert ResultDict to a more readable format
        output = {}
        for key, result_obj in result.items():
            output[key] = {
                "output": result_obj.output.tolist()
                if hasattr(result_obj.output, "tolist")
                else result_obj.output,
                "shape": getattr(result_obj.output, "shape", None),
            }
        return str(output)

    # This prevents the error:
    # PydanticUserError: `TypeAdapter[<function build_mcp_server.<locals>.execute_pipeline at ...>]`
    # is not fully defined; you should define `<function build_mcp_server.<locals>.execute_pipeline at ...>`
    # and all referenced types, then call `.rebuild()` on the instance.
    # This seems hacky, but it works. It also needs to be called inside of the body of this function.
    get_cached_typeadapter(execute_pipeline).json_schema()

    mcp.add_tool(execute_pipeline, name="execute_pipeline", description=description)
    return mcp


def run_mcp_server(
    pipeline: Pipeline,
    transport: Literal["stdio", "sse", "streamable-http"] = "stdio",
    version: str = "1.0.0",
) -> None:  # pragma: no cover
    """Run the MCP server using stdio transport."""
    mcp = build_mcp_server(pipeline, version=version)
    mcp.run(transport=transport)
