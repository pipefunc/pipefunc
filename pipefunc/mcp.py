"""Autogenerated MCP server for Pipefunc pipelines."""

import uuid
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Literal

import fastmcp
import pydantic

from pipefunc._pipeline._autodoc import PipelineDocumentation, format_pipeline_docs
from pipefunc._pipeline._base import Pipeline
from pipefunc._utils import requires
from pipefunc.map._mapspec import MapSpec
from pipefunc.map._run_eager_async import AsyncMap


@dataclass
class JobInfo:
    """Information about an async pipeline job."""

    runner: AsyncMap
    started_at: datetime
    run_folder: str
    status: Literal["running", "completed", "cancelled"]
    pipeline_name: str


# Global job registry to track async pipeline executions (UUID -> JobInfo)
job_registry: dict[str, JobInfo] = {}

_DEFAULT_PIPELINE_NAME = "Unnamed Pipeline"
_DEFAULT_PIPELINE_DESCRIPTION = "No description provided."

_PIPEFUNC_INSTRUCTIONS = """\
This MCP server executes pipefunc computational pipelines.
pipefunc creates function pipelines as DAGs where functions are automatically connected based on input/output dependencies.
See https://pipefunc.readthedocs.io/en/latest/ and https://github.com/pipefunc/pipefunc for more information.

<general>
CORE CONCEPTS:
- Pipeline: Sequence of interconnected functions forming a computational workflow
- MapSpec: String syntax defining how arrays map between functions

EXECUTION MODES:
Two execution modes are available:

1. **Synchronous Execution** (execute_pipeline_sync):
   - Blocks until completion and returns results immediately
   - Use when you need results right away for small-to-medium pipelines
   - Best for interactive use and when results fit in memory

2. **Asynchronous Execution** (execute_pipeline_async):
   - Returns immediately with a job ID for tracking
   - Use for long-running pipelines or when you need progress monitoring
   - Check progress with check_job_status, cancel with cancel_job
   - Results are retrieved when job completes
   - Best for large pipelines, batch processing, and background execution
   - IMPORTANT: Always IMMEDIATELY check the job status with check_job_status after starting a new job!

EXECUTION PARAMETERS:
- inputs: Dictionary with parameter values (single values or arrays)
- parallel: Boolean (default true) - enables parallel execution
- run_folder: Optional string - directory to save intermediate results

INPUT FORMATS:
1. Single values: {{"param1": 5, "param2": 10}} - executes once with these values
2. Array sweeps: {{"param1": [1,2,3], "param2": [4,5]}} - creates parameter combinations based on mapspec
3. Mixed: {{"data": [1,2,3], "constant": 10}} - arrays are swept, single values used for all iterations

MAPSPEC SYNTAX:
- General syntax: "input_name[index] -> output_name[index]"
- "x[i] -> y[i]": Element-wise processing (same array length)
- "a[i], b[j] -> result[i,j]": Cross-product (all combinations)
- "x[i], y[i] -> z[i]": Zipped processing (paired elements)
- "x[i, :] -> y[i]": Reduction across dimension
- "... -> x[i]": Dynamic array generation

MAPSPEC INDEX RULES:
- Same index letter ([i], [i]): Elements processed together (zipped)
- Different indices ([i], [j]): Create cross-product combinations
- No indices: Single values used for all iterations

OUTPUT FORMAT:
Returns dictionary with all pipeline outputs. Each output contains:
- "output": Computed result (converted to JSON-compatible format)
- "shape": Array dimensions (if applicable)

JOB MANAGEMENT:
- check_job_status: Monitor progress and get results when complete
- list_jobs: See all running/completed jobs
- cancel_job: Stop a running job

</general>

PIPELINE DESCRIPTION:
{pipeline_description}
"""

_PIPELINE_EXECUTE_DESCRIPTION_TEMPLATE = """\
Execute the pipeline with inputs.

PIPELINE NAME:
{pipeline_name}

PIPELINE DESCRIPTION:
{pipeline_description}

PIPELINE INFORMATION:
{pipeline_info}

MAPSPEC DEFINITIONS:
{mapspec_section}

INPUT FORMAT:
{input_format}

DETAILED PIPELINE DOCUMENTATION:
{documentation}
"""

_NO_MAPSPEC_INPUT_FORMAT = """\
Single values only:
  {"a": 5, "b": 10, "x": 2}
  → Each parameter gets a single value

This will execute the pipeline once with these specific values and return the result.
"""

_MAPSPEC_INPUT_FORMAT = """\

1. Simple element-wise mapping:
   {"x": [1, 2, 3, 4]}
   → If function has mapspec "x[i] -> y[i]", this will process each x value independently

2. Cross-product of inputs:
   {"a": [1, 2], "b": [10, 20]}
   → If functions have mapspecs like "a[i], b[j] -> result[i, j]", this creates all combinations

3. Zipped inputs (same index):
   {"x": [1, 2, 3], "y": [4, 5, 6]}
   → If function has mapspec "x[i], y[i] -> z[i]", this pairs x[0] with y[0], x[1] with y[1], etc.

4. Mixed single values and arrays:
   {"data": [1, 2, 3, 4], "multiplier": 10}
   → Arrays are mapped over, single values are used for all iterations

Key concepts:
- mapspec defines how inputs map to outputs (e.g., "x[i] -> y[i]" means element-wise)
- Arrays with same index letter (like [i]) are processed together
- Arrays with different indices (like [i] and [j]) create cross-products
- Single values work regardless of mapspecs
"""

_PIPELINE_ASYNC_EXECUTE_DESCRIPTION_EXTRA = """\
This tool returns a job ID and run folder.

Use the job ID to:
- check_job_status: Monitor progress and get results when complete
- list_jobs: See all running/completed jobs
- cancel_job: Stop a running job

IMPORTANT:
- Whenever starting a new job, ALWAYS immediately check the job status with check_job_status.
"""


def _get_pipeline_documentation(pipeline: Pipeline) -> str:
    """Generate formatted pipeline documentation tables using Rich."""
    requires("rich", "griffe", reason="mcp", extras="autodoc")
    from rich.console import Console

    doc = PipelineDocumentation.from_pipeline(pipeline)
    tables = format_pipeline_docs(doc, print_table=False)
    assert tables is not None

    console = Console(no_color=True)
    with console.capture() as capture:
        for table in tables:
            console.print(table, "\n")
    return capture.get()


def _get_pipeline_info_summary(pipeline_name: str, pipeline: Pipeline) -> str:
    """Generate a summary of pipeline information."""
    info = pipeline.info()
    assert info is not None

    def _format(key: str) -> str:
        return ", ".join(info[key]) if info[key] else "None"

    lines = [
        f"Pipeline Name: {pipeline_name}",
        f"Required Inputs: {_format('required_inputs')}",
        f"Optional Inputs: {_format('optional_inputs')}",
        f"Outputs: {_format('outputs')}",
        f"Intermediate Outputs: {_format('intermediate_outputs')}",
    ]
    return "\n".join(lines)


def _is_root_mapspec(mapspec: MapSpec, root_args: tuple[str, ...]) -> bool:
    """Check if a mapspec is a root mapspec."""
    return any(arg in mapspec.input_names for arg in root_args)


def _get_mapspec_section(pipeline: Pipeline) -> str:
    """Generate mapspec information section."""
    mapspecs = pipeline.mapspecs(ordered=True)
    if not mapspecs:
        return "None (This pipeline processes single values only)"

    lines = [
        "The following mapspecs define how arrays are processed:",
    ]
    root_args = pipeline.root_args()
    for i, mapspec in enumerate(mapspecs, 1):
        post = (
            " (used as input)" if _is_root_mapspec(mapspec, root_args) else " (intermediate step)"
        )
        lines.append(f"  {i}. {mapspec}{post}")

    lines.extend(
        [
            "",
            "Mapspec Legend:",
            "- Parameters with [i], [j], etc. represent array dimensions",
            "- Same index letter (e.g., [i]) means elements are processed together (zipped)",
            "- Different indices (e.g., [i] and [j]) create cross-products",
        ],
    )

    return "\n".join(lines)


def _get_input_format_section(pipeline: Pipeline) -> str:
    """Generate input format examples section."""
    mapspecs = pipeline.mapspecs_as_strings
    return _MAPSPEC_INPUT_FORMAT if mapspecs else _NO_MAPSPEC_INPUT_FORMAT


def _format_tool_description(pipeline: Pipeline) -> str:
    """Format a complete tool description using the template."""
    pipeline_name = pipeline.name or "Unnamed Pipeline"
    pipeline_description = pipeline.description or "No description provided."
    documentation = _get_pipeline_documentation(pipeline)
    pipeline_info = _get_pipeline_info_summary(pipeline_name, pipeline)
    mapspec_section = _get_mapspec_section(pipeline)
    input_format = _get_input_format_section(pipeline)
    return _PIPELINE_EXECUTE_DESCRIPTION_TEMPLATE.format(
        pipeline_name=pipeline_name,
        pipeline_description=pipeline_description,
        pipeline_info=pipeline_info,
        mapspec_section=mapspec_section,
        input_format=input_format,
        documentation=documentation,
    )


def build_mcp_server(pipeline: Pipeline, **fast_mcp_kwargs: Any) -> fastmcp.FastMCP:
    """Build an MCP (Model Context Protocol) server for a Pipefunc pipeline.

    This function creates a FastMCP server that exposes your Pipefunc pipeline as an
    MCP tool, allowing AI assistants and other MCP clients to execute your computational
    workflows. The server automatically generates parameter validation, documentation,
    and provides parallel execution capabilities.

    Parameters
    ----------
    pipeline
        A Pipefunc Pipeline object containing the computational workflow to expose.
        The pipeline's functions, parameters, and mapspecs will be automatically
        analyzed to generate the MCP tool interface.
    **fast_mcp_kwargs
        Additional keyword arguments to pass to the FastMCP server.
        See {class}`fastmcp.FastMCP` for more details.

    Returns
    -------
    fastmcp.FastMCP
        A configured FastMCP server instance ready to run. The server includes:

        - Automatic parameter validation using Pydantic models
        - Documentation based on the pipeline's functions doc-strings and parameter annotations
        - Synchronous and asynchronous execution capabilities
        - Job management for async pipeline execution with progress tracking
        - JSON-serializable output formatting

    Examples
    --------
    **Basic Usage:**

    Create and run an MCP server from a pipeline::

        # my_mcp.py
        from physics_pipeline import pipeline_charge  # import from module to enable correct serialization
        from pipefunc.mcp import build_mcp_server

        if __name__ == "__main__":  # Important to use this 'if' for parallel execution!
            mcp = build_mcp_server(pipeline_charge)
            mcp.run(path="/charge", port=8000, transport="streamable-http")

    **Client Configuration:**

    Register the server with an MCP client (e.g., Cursor IDE ``.cursor/mcp.json``)::

        {
          "mcpServers": {
            "physics-simulation": {
              "url": "http://127.0.0.1:8000/charge"
            }
          }
        }

    **Alternative Transport Methods:**

    .. code-block:: python

        # HTTP server (recommended for development)
        mcp = build_mcp_server(pipeline)
        mcp.run(path="/api", port=8000, transport="streamable-http")

        # Standard I/O (for CLI integration)
        mcp = build_mcp_server(pipeline)
        mcp.run(transport="stdio")

        # Server-Sent Events
        mcp = build_mcp_server(pipeline)
        mcp.run(transport="sse")

    **Pipeline Requirements:**

    Your pipeline should be properly configured with JSON serializable inputs
    with proper type annotations::

        from pipefunc import pipefunc, Pipeline

        @pipefunc(output_name="result")
        def calculate(x: float, y: float) -> float:
            return x * y + 2

        pipeline = Pipeline([calculate])
        mcp = build_mcp_server(pipeline)

    **Async Pipeline Execution:**

    The server provides tools for asynchronous pipeline execution with job management::

        # Start an async job
        execute_pipeline_async(inputs={"x": [1, 2, 3], "y": [4, 5, 6]})
        # Returns: {"job_id": "uuid-string", "run_folder": "runs/job_uuid-string"}

        # Check job status and progress
        check_job_status(job_id="uuid-string")
        # Returns status, progress, and results when complete

        # Cancel a running job
        cancel_job(job_id="uuid-string")

        # List all tracked jobs
        list_jobs()
        # Returns summary of all jobs with their status

    **Execution Modes:**

    The server provides two execution patterns:

    1. **Synchronous execution** (``execute_pipeline_sync``):
       Uses ``pipeline.map()`` - blocks until completion, returns results immediately.
       Best for small-to-medium pipelines when you need results right away.

    2. **Asynchronous execution** (``execute_pipeline_async``):
       Uses ``pipeline.map_async()`` - returns immediately with job tracking.
       Best for long-running pipelines, background processing, and when you need
       progress monitoring or cancellation capabilities.

    Notes
    -----
    - The server automatically handles type validation using the pipeline's Pydantic model
    - Output arrays are converted to JSON-compatible lists
    - Parallel execution is enabled by default but can be disabled per request
    - Async execution provides job management with progress tracking and cancellation capabilities
    - Job registry is maintained globally across all MCP tool calls

    See Also
    --------
    run_mcp_server : Convenience function to build and run server in one call
    Pipeline.map : The underlying method used to execute pipeline workflows

    """
    requires("mcp", "rich", "griffe", reason="mcp", extras="mcp")

    # Generate all pipeline information sections
    pipeline_name = pipeline.name or _DEFAULT_PIPELINE_NAME
    pipeline_description = pipeline.description or _DEFAULT_PIPELINE_DESCRIPTION

    # Format description using the template
    execute_pipeline_tool_description = _format_tool_description(pipeline)
    async_execute_pipeline_tool_description = (
        execute_pipeline_tool_description + "\n\n" + _PIPELINE_ASYNC_EXECUTE_DESCRIPTION_EXTRA
    )

    Model = pipeline.pydantic_model()  # noqa: N806
    Model.model_rebuild()  # Ensure all type references are resolved
    mcp = fastmcp.FastMCP(
        name=pipeline_name,
        instructions=_PIPEFUNC_INSTRUCTIONS.format(pipeline_description=pipeline_description),
        **fast_mcp_kwargs,
    )

    @mcp.tool(name="execute_pipeline_sync", description=execute_pipeline_tool_description)
    async def execute_pipeline_sync(
        ctx: fastmcp.Context,
        inputs: Model,  # type: ignore[valid-type]
        parallel: bool = True,  # noqa: FBT001, FBT002
        run_folder: str | None = None,
    ) -> str:
        """Execute pipeline synchronously and return results.

        This uses pipeline.map() which blocks until all computations are complete,
        then returns the final results. The function is async only to support
        ctx.info() calls for logging.
        """
        return await _execute_pipeline_sync(pipeline, ctx, inputs, parallel, run_folder)

    @mcp.tool(name="execute_pipeline_async", description=async_execute_pipeline_tool_description)
    async def execute_pipeline_async(
        ctx: fastmcp.Context,
        inputs: Model,  # type: ignore[valid-type]
        run_folder: str | None = None,
    ) -> str:
        """Start pipeline execution asynchronously and return job tracking info.

        This uses pipeline.map_async() which returns immediately with an AsyncMap
        object that can be tracked and awaited separately. The actual computation
        runs in the background.
        """
        return await _execute_pipeline_async(pipeline, ctx, inputs, run_folder)

    @mcp.tool(name="check_job_status")
    async def check_job_status(job_id: str) -> str:
        """Check status of a running pipeline job."""
        return await _check_job_status(job_id)

    @mcp.tool(name="cancel_job")
    async def cancel_job(ctx: fastmcp.Context, job_id: str) -> str:
        """Cancel a running pipeline job."""
        return await _cancel_job(ctx, job_id)

    @mcp.tool(name="list_jobs")
    async def list_jobs() -> str:
        """List all pipeline jobs with their current status."""
        return await _list_jobs()

    return mcp


async def _execute_pipeline_sync(
    pipeline: Pipeline,
    ctx: fastmcp.Context,
    inputs: pydantic.BaseModel,  # type: ignore[valid-type]
    parallel: bool = True,  # noqa: FBT001, FBT002
    run_folder: str | None = None,
) -> str:
    await ctx.info(f"Executing pipeline {pipeline.name=} with inputs: {inputs}")
    result = pipeline.map(
        inputs=inputs,
        parallel=parallel,
        run_folder=run_folder,
    )
    await ctx.info(f"Pipeline {pipeline.name=} executed")
    # Convert ResultDict to a more readable format
    output = {}
    for key, result_obj in result.items():
        output[key] = {
            "output": result_obj.output.tolist()
            if hasattr(result_obj.output, "tolist")
            else result_obj.output,
            "shape": getattr(result_obj.output, "shape", None),
        }
    return str(output)


async def _execute_pipeline_async(
    pipeline: Pipeline,
    ctx: fastmcp.Context,
    inputs: pydantic.BaseModel,  # type: ignore[valid-type]
    run_folder: str | None = None,
) -> str:
    job_id = str(uuid.uuid4())
    actual_run_folder = run_folder or f"runs/job_{job_id}"

    await ctx.info(
        f"Starting async pipeline {pipeline.name=} with job_id={job_id} and run_folder={actual_run_folder}",
    )

    # Store the AsyncMap object in the global registry
    async_map = pipeline.map_async(
        inputs=inputs,
        run_folder=actual_run_folder,
        show_progress="headless",
    )
    job_registry[job_id] = JobInfo(
        runner=async_map,
        started_at=datetime.now(tz=timezone.utc),
        run_folder=actual_run_folder,
        status="running",
        pipeline_name=pipeline.name or "Unnamed Pipeline",
    )

    await ctx.info(f"Started async job {job_id} in folder {actual_run_folder}")
    return str({"job_id": job_id, "run_folder": actual_run_folder})


async def _check_job_status(job_id: str) -> str:
    job = job_registry.get(job_id)
    if not job:
        return str({"error": "Job not found"})

    task = job.runner.task
    is_done = task.done()

    # Get progress information if available
    assert job.runner.progress is not None
    progress_dict = job.runner.progress.progress_dict
    progress_info = {}
    for output_name, status in progress_dict.items():
        elapsed_time = status.elapsed_time()
        remaining_time = status.remaining_time(elapsed_time=elapsed_time)
        progress_info[str(output_name)] = {
            "progress": status.progress,
            "n_completed": status.n_completed,
            "n_total": status.n_total,
            "n_failed": status.n_failed,
            "elapsed_time": elapsed_time if elapsed_time is not None else None,
            "remaining_time": remaining_time if remaining_time is not None else None,
        }

    result_info = {
        "job_id": job_id,
        "pipeline_name": job.pipeline_name,
        "status": "completed" if is_done else "running",
        "progress": progress_info,
        "run_folder": job.run_folder,
        "started_at": job.started_at.isoformat(),
        "error": str(task.exception()) if is_done and task.exception() else None,
    }

    # If job is completed, get the results
    if is_done and not task.exception():
        pipeline_result = task.result()
        output = {}
        for key, result_obj in pipeline_result.items():
            output[key] = {
                "output": result_obj.output.tolist()
                if hasattr(result_obj.output, "tolist")
                else result_obj.output,
                "shape": getattr(result_obj.output, "shape", None),
            }
        result_info["results"] = output

    return str(result_info)


async def _cancel_job(ctx: fastmcp.Context, job_id: str) -> str:
    job = job_registry.get(job_id)
    if not job:
        return str({"error": "Job not found"})

    task = job.runner.task
    if not task.done():
        task.cancel()
        job.status = "cancelled"
        await ctx.info(f"Cancelled job {job_id}")
        return str({"status": "cancelled", "job_id": job_id})
    return str({"error": "Job not found or already completed", "job_id": job_id})


async def _list_jobs() -> str:
    if not job_registry:
        return str({"jobs": [], "total_count": 0})

    jobs_info = []
    for job_id, job in job_registry.items():
        task = job.runner.task
        is_done = task.done()

        job_info = {
            "job_id": job_id,
            "pipeline_name": job.pipeline_name,
            "status": job.status,
            "run_folder": job.run_folder,
            "started_at": job.started_at.isoformat(),
            "has_error": is_done and task.exception() is not None,
        }
        jobs_info.append(job_info)

    return str({"jobs": jobs_info, "total_count": len(jobs_info)})
