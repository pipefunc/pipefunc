"""Autogenerated MCP server for Pipefunc pipelines."""

from typing import Any

import fastmcp

from pipefunc._pipeline._autodoc import PipelineDocumentation, format_pipeline_docs
from pipefunc._pipeline._base import Pipeline
from pipefunc._utils import requires

_PIPEFUNC_INSTRUCTIONS = """\
This MCP server executes Pipefunc computational pipelines.
Pipefunc creates function pipelines as DAGs where functions are automatically connected based on input/output dependencies.

CORE CONCEPTS:
- Pipeline: Sequence of interconnected functions forming a computational workflow
- MapSpec: String syntax defining how arrays map between functions
- Parallel Execution: Automatically parallelizes computations across parameter combinations
- Parameter Sweeps: Process multiple input combinations efficiently

EXECUTION PARAMETERS:
- input: Dictionary with parameter values (single values or arrays)
- parallel: Boolean (default true) - enables parallel execution
- run_folder: Optional string - directory to save intermediate results

INPUT FORMATS:
1. Single values: {{"param1": 5, "param2": 10}} - executes once with these values
2. Array sweeps: {{"param1": [1,2,3], "param2": [4,5]}} - creates parameter combinations based on mapspec
3. Mixed: {{"data": [1,2,3], "constant": 10}} - arrays are swept, single values used for all iterations

MAPSPEC SYNTAX:
- "x[i] -> y[i]": Element-wise processing (same array length)
- "a[i], b[j] -> result[i,j]": Cross-product (all combinations)
- "x[i], y[i] -> z[i]": Zipped processing (paired elements)
- "x[i,:] -> y[i]": Reduction across dimension
- "... -> x[i]": Dynamic array generation

INDEX RULES:
- Same index letter ([i], [i]): Elements processed together (zipped)
- Different indices ([i], [j]): Create cross-product combinations
- No indices: Single values used for all iterations

OUTPUT FORMAT:
Returns dictionary with all pipeline outputs. Each output contains:
- "output": Computed result (converted to JSON-compatible format)
- "shape": Array dimensions (if applicable)

EXECUTION MODES:
- parallel=true: Functions execute concurrently, faster for large sweeps
- parallel=false: Sequential execution, more reliable for custom objects, better for debugging

COMMON ISSUES:
- Shape mismatches: Verify mapspec definitions match expected array dimensions
- Type errors: Ensure input types match function parameter types

PIPELINE DESCRIPTION:
{pipeline_description}
"""

_PIPELINE_DESCRIPTION_TEMPLATE = """\
Execute the pipeline with input values. This method works for both single values and arrays/lists.

PIPELINE INFORMATION:
{pipeline_info}

MAPSPEC DEFINITIONS:
{mapspec_section}

INPUT FORMAT:
{input_format}

DETAILED PIPELINE DOCUMENTATION:
{documentation}
"""

_NO_MAPSPEC_INPUT_FORMAT = """\
Single values only:
  {"a": 5, "b": 10, "x": 2}
  → Each parameter gets a single value

This will execute the pipeline once with these specific values and return the result.
"""

_MAPSPEC_INPUT_FORMAT = """\

1. Simple element-wise mapping:
   {"x": [1, 2, 3, 4]}
   → If function has mapspec "x[i] -> y[i]", this will process each x value independently

2. Cross-product of inputs:
   {"a": [1, 2], "b": [10, 20]}
   → If functions have mapspecs like "a[i], b[j] -> result[i, j]", this creates all combinations

3. Zipped inputs (same index):
   {"x": [1, 2, 3], "y": [4, 5, 6]}
   → If function has mapspec "x[i], y[i] -> z[i]", this pairs x[0] with y[0], x[1] with y[1], etc.

4. Mixed single values and arrays:
   {"data": [1, 2, 3, 4], "multiplier": 10}
   → Arrays are mapped over, single values are used for all iterations

Key concepts:
- mapspec defines how inputs map to outputs (e.g., "x[i] -> y[i]" means element-wise)
- Arrays with same index letter (like [i]) are processed together
- Arrays with different indices (like [i] and [j]) create cross-products
- Single values work regardless of mapspecs
"""


def _get_pipeline_documentation(pipeline: Pipeline) -> str:
    """Generate formatted pipeline documentation tables using Rich."""
    requires("rich", "griffe", reason="mcp", extras="autodoc")
    from rich.console import Console

    doc = PipelineDocumentation.from_pipeline(pipeline)
    tables = format_pipeline_docs(doc, print_table=False)
    assert tables is not None

    console = Console(no_color=True)
    with console.capture() as capture:
        for table in tables:
            console.print(table, "\n")
    return capture.get()


def _get_pipeline_info_summary(pipeline_name: str, pipeline: Pipeline) -> str:
    """Generate a summary of pipeline information."""
    info = pipeline.info()
    assert info is not None

    def _format(key: str) -> str:
        return ", ".join(info[key]) if info[key] else "None"

    lines = [
        f"Pipeline Name: {pipeline_name}",
        f"Required Inputs: {_format('required_inputs')}",
        f"Optional Inputs: {_format('optional_inputs')}",
        f"Outputs: {_format('outputs')}",
        f"Intermediate Outputs: {_format('intermediate_outputs')}",
    ]
    return "\n".join(lines)


def _get_mapspec_section(pipeline: Pipeline) -> str:
    """Generate mapspec information section."""
    mapspecs = pipeline.mapspecs_as_strings
    if not mapspecs:
        return "None (This pipeline processes single values only)"

    lines = [
        "The following mapspecs define how arrays are processed:",
    ]

    for i, mapspec in enumerate(mapspecs, 1):
        lines.append(f"  {i}. {mapspec}")

    lines.extend(
        [
            "",
            "Mapspec Legend:",
            "- Parameters with [i], [j], etc. represent array dimensions",
            "- Same index letter (e.g., [i]) means elements are processed together (zipped)",
            "- Different indices (e.g., [i] and [j]) create cross-products",
            "- Parameters without indices are used as single values for all iterations",
        ],
    )

    return "\n".join(lines)


def _get_input_format_section(pipeline: Pipeline) -> str:
    """Generate input format examples section."""
    mapspecs = pipeline.mapspecs_as_strings
    return _MAPSPEC_INPUT_FORMAT if mapspecs else _NO_MAPSPEC_INPUT_FORMAT


def _format_tool_description(
    pipeline_info: str,
    mapspec_section: str,
    input_format: str,
    documentation: str,
) -> str:
    """Format a complete tool description using the template."""
    return _PIPELINE_DESCRIPTION_TEMPLATE.format(
        pipeline_info=pipeline_info,
        mapspec_section=mapspec_section,
        input_format=input_format,
        documentation=documentation,
    )


def build_mcp_server(pipeline: Pipeline, **fast_mcp_kwargs: Any) -> fastmcp.FastMCP:
    """Build an MCP (Model Context Protocol) server for a Pipefunc pipeline.

    This function creates a FastMCP server that exposes your Pipefunc pipeline as an
    MCP tool, allowing AI assistants and other MCP clients to execute your computational
    workflows. The server automatically generates parameter validation, documentation,
    and provides parallel execution capabilities.

    Parameters
    ----------
    pipeline
        A Pipefunc Pipeline object containing the computational workflow to expose.
        The pipeline's functions, parameters, and mapspecs will be automatically
        analyzed to generate the MCP tool interface.
    **fast_mcp_kwargs
        Additional keyword arguments to pass to the FastMCP server.
        See {class}`fastmcp.FastMCP` for more details.

    Returns
    -------
    fastmcp.FastMCP
        A configured FastMCP server instance ready to run. The server includes:

        - Automatic parameter validation using Pydantic models
        - Documentation based on the pipeline's functions doc-strings and parameter annotations
        - Parallel execution capabilities
        - JSON-serializable output formatting

    Examples
    --------
    **Basic Usage:**

    Create and run an MCP server from a pipeline::

        # my_mcp.py
        from physics_pipeline import pipeline_charge  # import from module to enable correct serialization
        from pipefunc.mcp import build_mcp_server

        if __name__ == "__main__":  # Important to use this 'if' for parallel execution!
            mcp = build_mcp_server(pipeline_charge)
            mcp.run(path="/charge", port=8000, transport="streamable-http")

    **Client Configuration:**

    Register the server with an MCP client (e.g., Cursor IDE ``.cursor/mcp.json``)::

        {
          "mcpServers": {
            "physics-simulation": {
              "url": "http://127.0.0.1:8000/charge"
            }
          }
        }

    **Alternative Transport Methods:**

    .. code-block:: python

        # HTTP server (recommended for development)
        mcp = build_mcp_server(pipeline)
        mcp.run(path="/api", port=8000, transport="streamable-http")

        # Standard I/O (for CLI integration)
        mcp = build_mcp_server(pipeline)
        mcp.run(transport="stdio")

        # Server-Sent Events
        mcp = build_mcp_server(pipeline)
        mcp.run(transport="sse")

    **Pipeline Requirements:**

    Your pipeline should be properly configured with JSON serializable inputs
    with proper type annotations::

        from pipefunc import pipefunc, Pipeline

        @pipefunc(output_name="result")
        def calculate(x: float, y: float) -> float:
            return x * y + 2

        pipeline = Pipeline([calculate])
        mcp = build_mcp_server(pipeline)

    Notes
    -----
    - The server automatically handles type validation using the pipeline's Pydantic model
    - Output arrays are converted to JSON-compatible lists
    - Parallel execution is enabled by default but can be disabled per request

    See Also
    --------
    run_mcp_server : Convenience function to build and run server in one call
    Pipeline.map : The underlying method used to execute pipeline workflows

    """
    requires("mcp", "rich", "griffe", reason="mcp", extras="mcp")

    # Generate all pipeline information sections
    pipeline_name = pipeline.name or "Unnamed Pipeline"
    documentation = _get_pipeline_documentation(pipeline)
    pipeline_info = _get_pipeline_info_summary(pipeline_name, pipeline)
    mapspec_section = _get_mapspec_section(pipeline)
    input_format = _get_input_format_section(pipeline)

    # Format description using the template
    description = _format_tool_description(
        pipeline_info=pipeline_info,
        mapspec_section=mapspec_section,
        input_format=input_format,
        documentation=documentation,
    )

    Model = pipeline.pydantic_model()  # noqa: N806
    Model.model_rebuild()  # Ensure all type references are resolved
    mcp = fastmcp.FastMCP(
        name=pipeline_name,
        instructions=_PIPEFUNC_INSTRUCTIONS.format(
            pipeline_description=pipeline.description or "No description provided.",
        ),
        **fast_mcp_kwargs,
    )

    @mcp.tool(name="execute_pipeline", description=description)
    async def execute_pipeline(
        ctx: fastmcp.Context,
        input: Model,  # type: ignore[valid-type] # noqa: A002
        parallel: bool = True,  # noqa: FBT001, FBT002
        run_folder: str | None = None,
    ) -> str:
        """Execute pipeline with input values (works for both single values and arrays)."""
        await ctx.info(f"Executing pipeline {pipeline.name=} with input: {input}")
        result = pipeline.map(
            inputs=input,
            parallel=parallel,
            run_folder=run_folder,
        )
        await ctx.info(f"Pipeline {pipeline.name=} executed")
        # Convert ResultDict to a more readable format
        output = {}
        for key, result_obj in result.items():
            output[key] = {
                "output": result_obj.output.tolist()
                if hasattr(result_obj.output, "tolist")
                else result_obj.output,
                "shape": getattr(result_obj.output, "shape", None),
            }
        return str(output)

    return mcp
